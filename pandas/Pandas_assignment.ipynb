{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a621cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bad8c3",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bb694f",
   "metadata": {},
   "source": [
    "**1. Sales Data Analysis Use Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "580545bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OrderID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>CustomerName</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Region</th>\n",
       "      <th>OrderDate</th>\n",
       "      <th>ProductCategory</th>\n",
       "      <th>SalesAmount</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Profit</th>\n",
       "      <th>SalesRep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>C001</td>\n",
       "      <td>Alice Johnson</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>East</td>\n",
       "      <td>2025-08-01</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>1200.50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.10</td>\n",
       "      <td>300.12</td>\n",
       "      <td>William Clark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>C002</td>\n",
       "      <td>Bob Smith</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>West</td>\n",
       "      <td>2025-08-03</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>450.75</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>90.15</td>\n",
       "      <td>Olivia Martinez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>C003</td>\n",
       "      <td>Carol Lee</td>\n",
       "      <td>Home Office</td>\n",
       "      <td>Central</td>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>Technology</td>\n",
       "      <td>890.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>150.50</td>\n",
       "      <td>James Wilson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>C004</td>\n",
       "      <td>David Kim</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>South</td>\n",
       "      <td>2025-08-07</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>670.80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.15</td>\n",
       "      <td>120.30</td>\n",
       "      <td>Olivia Martinez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>C005</td>\n",
       "      <td>Emma Davis</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>East</td>\n",
       "      <td>2025-08-09</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>310.25</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>70.40</td>\n",
       "      <td>William Clark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1006</td>\n",
       "      <td>C006</td>\n",
       "      <td>Frank Wilson</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>Central</td>\n",
       "      <td>2025-08-11</td>\n",
       "      <td>Technology</td>\n",
       "      <td>1500.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>250.00</td>\n",
       "      <td>James Wilson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1007</td>\n",
       "      <td>C007</td>\n",
       "      <td>Grace Chen</td>\n",
       "      <td>Home Office</td>\n",
       "      <td>West</td>\n",
       "      <td>2025-08-13</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>920.45</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>200.10</td>\n",
       "      <td>Olivia Martinez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1008</td>\n",
       "      <td>C008</td>\n",
       "      <td>Henry Patel</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>East</td>\n",
       "      <td>2025-08-15</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>540.60</td>\n",
       "      <td>6</td>\n",
       "      <td>0.10</td>\n",
       "      <td>130.50</td>\n",
       "      <td>William Clark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1009</td>\n",
       "      <td>C009</td>\n",
       "      <td>Irene Lopez</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>South</td>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>Technology</td>\n",
       "      <td>720.00</td>\n",
       "      <td>3</td>\n",
       "      <td>0.15</td>\n",
       "      <td>180.00</td>\n",
       "      <td>William Clark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1010</td>\n",
       "      <td>C010</td>\n",
       "      <td>Jack Brown</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>Central</td>\n",
       "      <td>2025-08-19</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>1100.80</td>\n",
       "      <td>4</td>\n",
       "      <td>0.20</td>\n",
       "      <td>250.00</td>\n",
       "      <td>Olivia Martinez</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OrderID CustomerID   CustomerName      Segment   Region   OrderDate  \\\n",
       "0     1001       C001  Alice Johnson     Consumer     East  2025-08-01   \n",
       "1     1002       C002      Bob Smith    Corporate     West  2025-08-03   \n",
       "2     1003       C003      Carol Lee  Home Office  Central  2025-08-05   \n",
       "3     1004       C004      David Kim     Consumer    South  2025-08-07   \n",
       "4     1005       C005     Emma Davis    Corporate     East  2025-08-09   \n",
       "5     1006       C006   Frank Wilson     Consumer  Central  2025-08-11   \n",
       "6     1007       C007     Grace Chen  Home Office     West  2025-08-13   \n",
       "7     1008       C008    Henry Patel     Consumer     East  2025-08-15   \n",
       "8     1009       C009    Irene Lopez    Corporate    South  2025-08-17   \n",
       "9     1010       C010     Jack Brown     Consumer  Central  2025-08-19   \n",
       "\n",
       "   ProductCategory  SalesAmount  Quantity  Discount  Profit         SalesRep  \n",
       "0        Furniture      1200.50         2      0.10  300.12    William Clark  \n",
       "1  Office Supplies       450.75         5      0.05   90.15  Olivia Martinez  \n",
       "2       Technology       890.00         1      0.20  150.50     James Wilson  \n",
       "3        Furniture       670.80         3      0.15  120.30  Olivia Martinez  \n",
       "4  Office Supplies       310.25         4      0.00   70.40    William Clark  \n",
       "5       Technology      1500.00         2      0.25  250.00     James Wilson  \n",
       "6        Furniture       920.45         1      0.05  200.10  Olivia Martinez  \n",
       "7  Office Supplies       540.60         6      0.10  130.50    William Clark  \n",
       "8       Technology       720.00         3      0.15  180.00    William Clark  \n",
       "9        Furniture      1100.80         4      0.20  250.00  Olivia Martinez  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_data = {\n",
    "    \"OrderID\": [1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010],\n",
    "    \"CustomerID\": ['C001', 'C002', 'C003', 'C004', 'C005', 'C006', 'C007', 'C008', 'C009', 'C010'],\n",
    "    \"CustomerName\": [\n",
    "        'Alice Johnson', 'Bob Smith', 'Carol Lee', 'David Kim', 'Emma Davis',\n",
    "        'Frank Wilson', 'Grace Chen', 'Henry Patel', 'Irene Lopez', 'Jack Brown'\n",
    "    ],\n",
    "    \"Segment\": [\n",
    "        'Consumer', 'Corporate', 'Home Office', 'Consumer', 'Corporate',\n",
    "        'Consumer', 'Home Office', 'Consumer', 'Corporate', 'Consumer'\n",
    "    ],\n",
    "    \"Region\": [\n",
    "        'East', 'West', 'Central', 'South', 'East',\n",
    "        'Central', 'West', 'East', 'South', 'Central'\n",
    "    ],\n",
    "    \"OrderDate\": [\n",
    "        '2025-08-01', '2025-08-03', '2025-08-05', '2025-08-07', '2025-08-09',\n",
    "        '2025-08-11', '2025-08-13', '2025-08-15', '2025-08-17', '2025-08-19'\n",
    "    ],\n",
    "    \"ProductCategory\": [\n",
    "        'Furniture', 'Office Supplies', 'Technology', 'Furniture', 'Office Supplies',\n",
    "        'Technology', 'Furniture', 'Office Supplies', 'Technology', 'Furniture'\n",
    "    ],\n",
    "    \"SalesAmount\": [1200.50, 450.75, 890.00, 670.80, 310.25, 1500.00, 920.45, 540.60, 720.00, 1100.80],\n",
    "    \"Quantity\": [2, 5, 1, 3, 4, 2, 1, 6, 3, 4],\n",
    "    \"Discount\": [0.10, 0.05, 0.20, 0.15, 0.00, 0.25, 0.05, 0.10, 0.15, 0.20],\n",
    "    \"Profit\": [300.12, 90.15, 150.50, 120.30, 70.40, 250.00, 200.10, 130.50, 180.00, 250.00],\n",
    "    \"SalesRep\": [\n",
    "        'William Clark', 'Olivia Martinez', 'James Wilson', 'Olivia Martinez', 'William Clark',\n",
    "        'James Wilson', 'Olivia Martinez', 'William Clark', 'William Clark', 'Olivia Martinez'\n",
    "    ]\n",
    "}\n",
    "data = pd.DataFrame(sales_data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c99ea9",
   "metadata": {},
   "source": [
    "Count total sales per product category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b4061516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProductCategory\n",
       "Furniture          4\n",
       "Office Supplies    3\n",
       "Technology         3\n",
       "Name: OrderID, dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(\"ProductCategory\")[\"OrderID\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee0e542",
   "metadata": {},
   "source": [
    "Calculate the total revenue generated by each sales representative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3350275e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SalesRep\n",
       "James Wilson       400.50\n",
       "Olivia Martinez    660.55\n",
       "William Clark      681.02\n",
       "Name: Profit, dtype: float64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('SalesRep')['Profit'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1fc182",
   "metadata": {},
   "source": [
    "Find the product with the highest sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "886df0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Office Supplies'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[data['Quantity'].idxmax()]['ProductCategory']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efaee71",
   "metadata": {},
   "source": [
    "Group data by sales regions and calculate average sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b2b7c548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Region\n",
       "Central    1163.600000\n",
       "East        683.783333\n",
       "South       695.400000\n",
       "West        685.600000\n",
       "Name: SalesAmount, dtype: float64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['product_of_two'] = data['Profit']*data['Quantity']###\n",
    "data.groupby('Region')['product_of_two'].mean()\n",
    "\n",
    "data.groupby('Region')['SalesAmount'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07302169",
   "metadata": {},
   "source": [
    "---\n",
    "**2. Employee Data Analysis Use Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4db7fd58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Department</th>\n",
       "      <th>Salary</th>\n",
       "      <th>PerformanceScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E001</td>\n",
       "      <td>Alice Brown</td>\n",
       "      <td>HR</td>\n",
       "      <td>55000</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E002</td>\n",
       "      <td>Bob Smith</td>\n",
       "      <td>IT</td>\n",
       "      <td>72000</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E003</td>\n",
       "      <td>Carol Lee</td>\n",
       "      <td>Finance</td>\n",
       "      <td>68000</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E004</td>\n",
       "      <td>David Kim</td>\n",
       "      <td>IT</td>\n",
       "      <td>75000</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E005</td>\n",
       "      <td>Emma Davis</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>50000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>E006</td>\n",
       "      <td>Frank Wilson</td>\n",
       "      <td>HR</td>\n",
       "      <td>53000</td>\n",
       "      <td>7.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>E007</td>\n",
       "      <td>Grace Chen</td>\n",
       "      <td>Finance</td>\n",
       "      <td>67000</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>E008</td>\n",
       "      <td>Henry Patel</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>52000</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>E009</td>\n",
       "      <td>Irene Lopez</td>\n",
       "      <td>IT</td>\n",
       "      <td>71000</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>E010</td>\n",
       "      <td>Jack Brown</td>\n",
       "      <td>HR</td>\n",
       "      <td>56000</td>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  EmployeeID          Name Department  Salary  PerformanceScore\n",
       "0       E001   Alice Brown         HR   55000               8.5\n",
       "1       E002     Bob Smith         IT   72000               7.2\n",
       "2       E003     Carol Lee    Finance   68000               9.0\n",
       "3       E004     David Kim         IT   75000               6.5\n",
       "4       E005    Emma Davis  Marketing   50000               8.0\n",
       "5       E006  Frank Wilson         HR   53000               7.8\n",
       "6       E007    Grace Chen    Finance   67000               8.9\n",
       "7       E008   Henry Patel  Marketing   52000               7.5\n",
       "8       E009   Irene Lopez         IT   71000               9.3\n",
       "9       E010    Jack Brown         HR   56000               6.8"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employee_data_dict = {\n",
    "    \"EmployeeID\": ['E001','E002','E003','E004','E005','E006','E007','E008','E009','E010'],\n",
    "    \"Name\": [\n",
    "        'Alice Brown', 'Bob Smith', 'Carol Lee', 'David Kim', 'Emma Davis',\n",
    "        'Frank Wilson', 'Grace Chen', 'Henry Patel', 'Irene Lopez', 'Jack Brown'\n",
    "    ],\n",
    "    \"Department\": [\n",
    "        'HR', 'IT', 'Finance', 'IT', 'Marketing',\n",
    "        'HR', 'Finance', 'Marketing', 'IT', 'HR'\n",
    "    ],\n",
    "    \"Salary\": [55000, 72000, 68000, 75000, 50000, 53000, 67000, 52000, 71000, 56000],\n",
    "    \"PerformanceScore\": [8.5, 7.2, 9.0, 6.5, 8.0, 7.8, 8.9, 7.5, 9.3, 6.8]\n",
    "}\n",
    "data = pd.DataFrame(employee_data_dict)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07996a30",
   "metadata": {},
   "source": [
    "• Count the number of employees per department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "887db602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Department\n",
       "Finance      2\n",
       "HR           3\n",
       "IT           3\n",
       "Marketing    2\n",
       "Name: EmployeeID, dtype: int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('Department')['EmployeeID'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebdd90a",
   "metadata": {},
   "source": [
    "• Find the employee with the highest salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dad091c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmployeeID               E004\n",
       "Name                David Kim\n",
       "Department                 IT\n",
       "Salary                  75000\n",
       "PerformanceScore          6.5\n",
       "Name: 3, dtype: object"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[data['Salary'].idxmax()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b082e4",
   "metadata": {},
   "source": [
    "• Calculate average salary per department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bd413d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Department\n",
       "Finance      67500.000000\n",
       "HR           54666.666667\n",
       "IT           72666.666667\n",
       "Marketing    51000.000000\n",
       "Name: Salary, dtype: float64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('Department')['Salary'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15c88da",
   "metadata": {},
   "source": [
    "Sort employees based on their performance score or salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ae3648fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Department</th>\n",
       "      <th>Salary</th>\n",
       "      <th>PerformanceScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E004</td>\n",
       "      <td>David Kim</td>\n",
       "      <td>IT</td>\n",
       "      <td>75000</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>E010</td>\n",
       "      <td>Jack Brown</td>\n",
       "      <td>HR</td>\n",
       "      <td>56000</td>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E002</td>\n",
       "      <td>Bob Smith</td>\n",
       "      <td>IT</td>\n",
       "      <td>72000</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>E008</td>\n",
       "      <td>Henry Patel</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>52000</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>E006</td>\n",
       "      <td>Frank Wilson</td>\n",
       "      <td>HR</td>\n",
       "      <td>53000</td>\n",
       "      <td>7.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E005</td>\n",
       "      <td>Emma Davis</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>50000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E001</td>\n",
       "      <td>Alice Brown</td>\n",
       "      <td>HR</td>\n",
       "      <td>55000</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>E007</td>\n",
       "      <td>Grace Chen</td>\n",
       "      <td>Finance</td>\n",
       "      <td>67000</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E003</td>\n",
       "      <td>Carol Lee</td>\n",
       "      <td>Finance</td>\n",
       "      <td>68000</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>E009</td>\n",
       "      <td>Irene Lopez</td>\n",
       "      <td>IT</td>\n",
       "      <td>71000</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  EmployeeID          Name Department  Salary  PerformanceScore\n",
       "3       E004     David Kim         IT   75000               6.5\n",
       "9       E010    Jack Brown         HR   56000               6.8\n",
       "1       E002     Bob Smith         IT   72000               7.2\n",
       "7       E008   Henry Patel  Marketing   52000               7.5\n",
       "5       E006  Frank Wilson         HR   53000               7.8\n",
       "4       E005    Emma Davis  Marketing   50000               8.0\n",
       "0       E001   Alice Brown         HR   55000               8.5\n",
       "6       E007    Grace Chen    Finance   67000               8.9\n",
       "2       E003     Carol Lee    Finance   68000               9.0\n",
       "8       E009   Irene Lopez         IT   71000               9.3"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_values('PerformanceScore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfca42d",
   "metadata": {},
   "source": [
    "----\n",
    "**3. Student Performance Analysis Use Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b98690df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StudentID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Math</th>\n",
       "      <th>Science</th>\n",
       "      <th>English</th>\n",
       "      <th>History</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S001</td>\n",
       "      <td>Alice Brown</td>\n",
       "      <td>85</td>\n",
       "      <td>78</td>\n",
       "      <td>92</td>\n",
       "      <td>74</td>\n",
       "      <td>Class A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S002</td>\n",
       "      <td>Bob Smith</td>\n",
       "      <td>58</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>66</td>\n",
       "      <td>Class A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S003</td>\n",
       "      <td>Carol Lee</td>\n",
       "      <td>95</td>\n",
       "      <td>18</td>\n",
       "      <td>82</td>\n",
       "      <td>91</td>\n",
       "      <td>Class B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S004</td>\n",
       "      <td>David Kim</td>\n",
       "      <td>76</td>\n",
       "      <td>85</td>\n",
       "      <td>20</td>\n",
       "      <td>79</td>\n",
       "      <td>Class B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S005</td>\n",
       "      <td>Emma Davis</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>68</td>\n",
       "      <td>70</td>\n",
       "      <td>Class A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>S006</td>\n",
       "      <td>Frank Wilson</td>\n",
       "      <td>90</td>\n",
       "      <td>95</td>\n",
       "      <td>88</td>\n",
       "      <td>92</td>\n",
       "      <td>Class C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>S007</td>\n",
       "      <td>Grace Chen</td>\n",
       "      <td>72</td>\n",
       "      <td>10</td>\n",
       "      <td>75</td>\n",
       "      <td>65</td>\n",
       "      <td>Class C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>S008</td>\n",
       "      <td>Henry Patel</td>\n",
       "      <td>81</td>\n",
       "      <td>79</td>\n",
       "      <td>85</td>\n",
       "      <td>80</td>\n",
       "      <td>Class B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S009</td>\n",
       "      <td>Irene Lopez</td>\n",
       "      <td>69</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>71</td>\n",
       "      <td>Class A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>S010</td>\n",
       "      <td>Jack Brown</td>\n",
       "      <td>88</td>\n",
       "      <td>10</td>\n",
       "      <td>86</td>\n",
       "      <td>19</td>\n",
       "      <td>Class C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  StudentID          Name  Math  Science  English  History    Class\n",
       "0      S001   Alice Brown    85       78       92       74  Class A\n",
       "1      S002     Bob Smith    58       64       10       66  Class A\n",
       "2      S003     Carol Lee    95       18       82       91  Class B\n",
       "3      S004     David Kim    76       85       20       79  Class B\n",
       "4      S005    Emma Davis    66       72       68       70  Class A\n",
       "5      S006  Frank Wilson    90       95       88       92  Class C\n",
       "6      S007    Grace Chen    72       10       75       65  Class C\n",
       "7      S008   Henry Patel    81       79       85       80  Class B\n",
       "8      S009   Irene Lopez    69       14       17       71  Class A\n",
       "9      S010    Jack Brown    88       10       86       19  Class C"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_data_dict = {\n",
    "    \"StudentID\": ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010'],\n",
    "    \"Name\": [\n",
    "        'Alice Brown', 'Bob Smith', 'Carol Lee', 'David Kim', 'Emma Davis',\n",
    "        'Frank Wilson', 'Grace Chen', 'Henry Patel', 'Irene Lopez', 'Jack Brown'\n",
    "    ],\n",
    "    \"Math\": [85, 58, 95, 76, 66, 90, 72, 81, 69, 88],\n",
    "    \"Science\": [78, 64, 18, 85, 72, 95, 10, 79, 14, 10],\n",
    "    \"English\": [92, 10, 82, 20, 68, 88, 75, 85, 17, 86],\n",
    "    \"History\": [74, 66, 91, 79, 70, 92, 65, 80, 71, 19],\n",
    "    \"Class\": ['Class A', 'Class A', 'Class B', 'Class B', 'Class A', 'Class C', 'Class C', 'Class B', 'Class A', 'Class C']\n",
    "}\n",
    "data = pd.DataFrame(student_data_dict)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b4bb380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['total'] = data.iloc[:,2:-1].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343c0ea7",
   "metadata": {},
   "source": [
    "• Count the number of students per class/grade.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b54fb143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "Class A    4\n",
       "Class B    3\n",
       "Class C    3\n",
       "Name: StudentID, dtype: int64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('Class')['StudentID'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cab6e8",
   "metadata": {},
   "source": [
    "• Calculate the average marks per subject.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c3759e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Math       78.0\n",
       "Science    52.5\n",
       "English    62.3\n",
       "History    70.7\n",
       "dtype: float64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[:,2:-2].mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1f0bee",
   "metadata": {},
   "source": [
    "• Find the student with the highest overall score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "48c156ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudentID            S006\n",
       "Name         Frank Wilson\n",
       "Math                   90\n",
       "Science                95\n",
       "English                88\n",
       "History                92\n",
       "Class             Class C\n",
       "total                 365\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[data['total'].idxmax()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08677f95",
   "metadata": {},
   "source": [
    "• Identify students who scored below the passing mark in any subject.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "96fab897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StudentID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Math</th>\n",
       "      <th>Science</th>\n",
       "      <th>English</th>\n",
       "      <th>History</th>\n",
       "      <th>Class</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S002</td>\n",
       "      <td>Bob Smith</td>\n",
       "      <td>58</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>66</td>\n",
       "      <td>Class A</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S003</td>\n",
       "      <td>Carol Lee</td>\n",
       "      <td>95</td>\n",
       "      <td>18</td>\n",
       "      <td>82</td>\n",
       "      <td>91</td>\n",
       "      <td>Class B</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S004</td>\n",
       "      <td>David Kim</td>\n",
       "      <td>76</td>\n",
       "      <td>85</td>\n",
       "      <td>20</td>\n",
       "      <td>79</td>\n",
       "      <td>Class B</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>S007</td>\n",
       "      <td>Grace Chen</td>\n",
       "      <td>72</td>\n",
       "      <td>10</td>\n",
       "      <td>75</td>\n",
       "      <td>65</td>\n",
       "      <td>Class C</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S009</td>\n",
       "      <td>Irene Lopez</td>\n",
       "      <td>69</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>71</td>\n",
       "      <td>Class A</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>S010</td>\n",
       "      <td>Jack Brown</td>\n",
       "      <td>88</td>\n",
       "      <td>10</td>\n",
       "      <td>86</td>\n",
       "      <td>19</td>\n",
       "      <td>Class C</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  StudentID         Name  Math  Science  English  History    Class  total\n",
       "1      S002    Bob Smith    58       64       10       66  Class A    198\n",
       "2      S003    Carol Lee    95       18       82       91  Class B    286\n",
       "3      S004    David Kim    76       85       20       79  Class B    260\n",
       "6      S007   Grace Chen    72       10       75       65  Class C    222\n",
       "8      S009  Irene Lopez    69       14       17       71  Class A    171\n",
       "9      S010   Jack Brown    88       10       86       19  Class C    203"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[(data.iloc[:,2:-2]<40).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb868dcd",
   "metadata": {},
   "source": [
    "---\n",
    "**4. E-commerce Orders Analysis Use Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "dd8972bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OrderID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>CustomerName</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>C001</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Laptop</td>\n",
       "      <td>1</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>C002</td>\n",
       "      <td>Bob</td>\n",
       "      <td>Mouse</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>C003</td>\n",
       "      <td>Carol</td>\n",
       "      <td>Keyboard</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>C001</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Monitor</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>C004</td>\n",
       "      <td>David</td>\n",
       "      <td>Laptop</td>\n",
       "      <td>2</td>\n",
       "      <td>1150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106</td>\n",
       "      <td>C005</td>\n",
       "      <td>Emma</td>\n",
       "      <td>Mouse</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>107</td>\n",
       "      <td>C002</td>\n",
       "      <td>Bob</td>\n",
       "      <td>Laptop</td>\n",
       "      <td>1</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>108</td>\n",
       "      <td>C003</td>\n",
       "      <td>Carol</td>\n",
       "      <td>Keyboard</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>109</td>\n",
       "      <td>C001</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Mouse</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>110</td>\n",
       "      <td>C004</td>\n",
       "      <td>David</td>\n",
       "      <td>Monitor</td>\n",
       "      <td>1</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>111</td>\n",
       "      <td>C005</td>\n",
       "      <td>Emma</td>\n",
       "      <td>Keyboard</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>112</td>\n",
       "      <td>C006</td>\n",
       "      <td>Frank</td>\n",
       "      <td>Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>113</td>\n",
       "      <td>C007</td>\n",
       "      <td>Grace</td>\n",
       "      <td>Smartphone</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>114</td>\n",
       "      <td>C008</td>\n",
       "      <td>Henry</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>115</td>\n",
       "      <td>C001</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Laptop</td>\n",
       "      <td>1</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>116</td>\n",
       "      <td>C002</td>\n",
       "      <td>Bob</td>\n",
       "      <td>Headphones</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>117</td>\n",
       "      <td>C003</td>\n",
       "      <td>Carol</td>\n",
       "      <td>Keyboard</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>118</td>\n",
       "      <td>C006</td>\n",
       "      <td>Frank</td>\n",
       "      <td>Smartphone</td>\n",
       "      <td>1</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>119</td>\n",
       "      <td>C007</td>\n",
       "      <td>Grace</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>2</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>120</td>\n",
       "      <td>C008</td>\n",
       "      <td>Henry</td>\n",
       "      <td>Monitor</td>\n",
       "      <td>1</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    OrderID CustomerID CustomerName     Product  Quantity  Price\n",
       "0       101       C001        Alice      Laptop         1   1200\n",
       "1       102       C002          Bob       Mouse         2     25\n",
       "2       103       C003        Carol    Keyboard         1     45\n",
       "3       104       C001        Alice     Monitor         1    200\n",
       "4       105       C004        David      Laptop         2   1150\n",
       "5       106       C005         Emma       Mouse         3     20\n",
       "6       107       C002          Bob      Laptop         1   1250\n",
       "7       108       C003        Carol    Keyboard         2     50\n",
       "8       109       C001        Alice       Mouse         1     25\n",
       "9       110       C004        David     Monitor         1    210\n",
       "10      111       C005         Emma    Keyboard         2     45\n",
       "11      112       C006        Frank  Headphones         1    100\n",
       "12      113       C007        Grace  Smartphone         1    800\n",
       "13      114       C008        Henry      Tablet         1    400\n",
       "14      115       C001        Alice      Laptop         1   1300\n",
       "15      116       C002          Bob  Headphones         2    100\n",
       "16      117       C003        Carol    Keyboard         1     50\n",
       "17      118       C006        Frank  Smartphone         1    750\n",
       "18      119       C007        Grace      Tablet         2    420\n",
       "19      120       C008        Henry     Monitor         1    210"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ecommerce_orders_dict = {\n",
    "    \"OrderID\": list(range(101, 121)),\n",
    "    \"CustomerID\": ['C001', 'C002', 'C003', 'C001', 'C004', 'C005', 'C002', 'C003', 'C001', 'C004',\n",
    "                   'C005', 'C006', 'C007', 'C008', 'C001', 'C002', 'C003', 'C006', 'C007', 'C008'],\n",
    "    \"CustomerName\": ['Alice', 'Bob', 'Carol', 'Alice', 'David', 'Emma', 'Bob', 'Carol', 'Alice', 'David',\n",
    "                     'Emma', 'Frank', 'Grace', 'Henry', 'Alice', 'Bob', 'Carol', 'Frank', 'Grace', 'Henry'],\n",
    "    \"Product\": ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Laptop', 'Mouse', 'Laptop', 'Keyboard', 'Mouse', 'Monitor',\n",
    "                'Keyboard', 'Headphones', 'Smartphone', 'Tablet', 'Laptop', 'Headphones', 'Keyboard', 'Smartphone', 'Tablet', 'Monitor'],\n",
    "    \"Quantity\": [1, 2, 1, 1, 2, 3, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1],\n",
    "    \"Price\": [1200, 25, 45, 200, 1150, 20, 1250, 50, 25, 210, 45, 100, 800, 400, 1300, 100, 50, 750, 420, 210]\n",
    "}\n",
    "\n",
    "data = pd.DataFrame(ecommerce_orders_dict)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860488d4",
   "metadata": {},
   "source": [
    "• Count total orders per customer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2b8495c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomerID\n",
       "C001    4\n",
       "C002    3\n",
       "C003    3\n",
       "C004    2\n",
       "C005    2\n",
       "C006    2\n",
       "C007    2\n",
       "C008    2\n",
       "Name: OrderID, dtype: int64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('CustomerID')['OrderID'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6baf8",
   "metadata": {},
   "source": [
    "• Find the customer who placed the highest number of orders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "67b8d20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alice'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('CustomerName')['Quantity'].count().idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5b9907",
   "metadata": {},
   "source": [
    "• Calculate revenue generated per product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "96a67cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['totalamount'] = data['Quantity']*data['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7eabe50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product\n",
       "Headphones     300\n",
       "Keyboard       285\n",
       "Laptop        6050\n",
       "Monitor        620\n",
       "Mouse          135\n",
       "Smartphone    1550\n",
       "Tablet        1240\n",
       "Name: totalamount, dtype: int64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('Product')['totalamount'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8dfafb",
   "metadata": {},
   "source": [
    "• Identify the top 5 selling products.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3e89b10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product\n",
       "Keyboard      6\n",
       "Mouse         6\n",
       "Laptop        5\n",
       "Headphones    3\n",
       "Monitor       3\n",
       "Name: Quantity, dtype: int64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('Product')['Quantity'].sum().sort_values(ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b02ec09",
   "metadata": {},
   "source": [
    "---\n",
    "**5. Hospital Patient Records Analysis Use Case**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4b79c5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatientID</th>\n",
       "      <th>PatientName</th>\n",
       "      <th>Department</th>\n",
       "      <th>AdmissionDate</th>\n",
       "      <th>DischargeDate</th>\n",
       "      <th>HospitalStayDays</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P001</td>\n",
       "      <td>Alice Brown</td>\n",
       "      <td>Cardiology</td>\n",
       "      <td>2025-01-05</td>\n",
       "      <td>2025-01-12</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P002</td>\n",
       "      <td>Bob Smith</td>\n",
       "      <td>Neurology</td>\n",
       "      <td>2025-01-10</td>\n",
       "      <td>2025-01-20</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P003</td>\n",
       "      <td>Carol Lee</td>\n",
       "      <td>Orthopedics</td>\n",
       "      <td>2025-02-15</td>\n",
       "      <td>2025-02-25</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P004</td>\n",
       "      <td>David Kim</td>\n",
       "      <td>Cardiology</td>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>2025-03-10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P005</td>\n",
       "      <td>Emma Davis</td>\n",
       "      <td>Pediatrics</td>\n",
       "      <td>2025-03-05</td>\n",
       "      <td>2025-03-12</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P006</td>\n",
       "      <td>Frank Wilson</td>\n",
       "      <td>Neurology</td>\n",
       "      <td>2025-04-12</td>\n",
       "      <td>2025-04-22</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P007</td>\n",
       "      <td>Grace Chen</td>\n",
       "      <td>Orthopedics</td>\n",
       "      <td>2025-04-20</td>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P008</td>\n",
       "      <td>Henry Patel</td>\n",
       "      <td>Cardiology</td>\n",
       "      <td>2025-05-03</td>\n",
       "      <td>2025-05-10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P009</td>\n",
       "      <td>Irene Lopez</td>\n",
       "      <td>Pediatrics</td>\n",
       "      <td>2025-05-10</td>\n",
       "      <td>2025-05-15</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>P010</td>\n",
       "      <td>Jack Brown</td>\n",
       "      <td>Neurology</td>\n",
       "      <td>2025-06-15</td>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>P011</td>\n",
       "      <td>Laura White</td>\n",
       "      <td>Orthopedics</td>\n",
       "      <td>2025-06-20</td>\n",
       "      <td>2025-06-28</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>P012</td>\n",
       "      <td>Mike Green</td>\n",
       "      <td>Cardiology</td>\n",
       "      <td>2025-07-01</td>\n",
       "      <td>2025-07-10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PatientID   PatientName   Department AdmissionDate DischargeDate  \\\n",
       "0       P001   Alice Brown   Cardiology    2025-01-05    2025-01-12   \n",
       "1       P002     Bob Smith    Neurology    2025-01-10    2025-01-20   \n",
       "2       P003     Carol Lee  Orthopedics    2025-02-15    2025-02-25   \n",
       "3       P004     David Kim   Cardiology    2025-03-01    2025-03-10   \n",
       "4       P005    Emma Davis   Pediatrics    2025-03-05    2025-03-12   \n",
       "5       P006  Frank Wilson    Neurology    2025-04-12    2025-04-22   \n",
       "6       P007    Grace Chen  Orthopedics    2025-04-20    2025-04-28   \n",
       "7       P008   Henry Patel   Cardiology    2025-05-03    2025-05-10   \n",
       "8       P009   Irene Lopez   Pediatrics    2025-05-10    2025-05-15   \n",
       "9       P010    Jack Brown    Neurology    2025-06-15    2025-06-25   \n",
       "10      P011   Laura White  Orthopedics    2025-06-20    2025-06-28   \n",
       "11      P012    Mike Green   Cardiology    2025-07-01    2025-07-10   \n",
       "\n",
       "    HospitalStayDays  \n",
       "0                  7  \n",
       "1                 10  \n",
       "2                 10  \n",
       "3                  9  \n",
       "4                  7  \n",
       "5                 10  \n",
       "6                  8  \n",
       "7                  7  \n",
       "8                  5  \n",
       "9                 10  \n",
       "10                 8  \n",
       "11                 9  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hospital_data_dict = {\n",
    "    \"PatientID\": ['P001', 'P002', 'P003', 'P004', 'P005', 'P006', 'P007', 'P008', 'P009', 'P010', 'P011', 'P012'],\n",
    "    \"PatientName\": ['Alice Brown', 'Bob Smith', 'Carol Lee', 'David Kim', 'Emma Davis', 'Frank Wilson', \n",
    "                    'Grace Chen', 'Henry Patel', 'Irene Lopez', 'Jack Brown', 'Laura White', 'Mike Green'],\n",
    "    \"Department\": ['Cardiology', 'Neurology', 'Orthopedics', 'Cardiology', 'Pediatrics', 'Neurology', \n",
    "                   'Orthopedics', 'Cardiology', 'Pediatrics', 'Neurology', 'Orthopedics', 'Cardiology'],\n",
    "    \"AdmissionDate\": ['2025-01-05', '2025-01-10', '2025-02-15', '2025-03-01', '2025-03-05', '2025-04-12', \n",
    "                      '2025-04-20', '2025-05-03', '2025-05-10', '2025-06-15', '2025-06-20', '2025-07-01'],\n",
    "    \"DischargeDate\": ['2025-01-12', '2025-01-20', '2025-02-25', '2025-03-10', '2025-03-12', '2025-04-22', \n",
    "                      '2025-04-28', '2025-05-10', '2025-05-15', '2025-06-25', '2025-06-28', '2025-07-10'],\n",
    "    \"HospitalStayDays\": [7, 10, 10, 9, 7, 10, 8, 7, 5, 10, 8, 9]\n",
    "}\n",
    "\n",
    "data = pd.DataFrame(hospital_data_dict)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99866426",
   "metadata": {},
   "source": [
    "**Count the number of patients per department.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2063eeca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Department\n",
       "Cardiology     4\n",
       "Neurology      3\n",
       "Orthopedics    3\n",
       "Pediatrics     2\n",
       "Name: PatientID, dtype: int64"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('Department')['PatientID'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2089a4",
   "metadata": {},
   "source": [
    "• Find the average hospital stay duration per department.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e883b5a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Department\n",
       "Cardiology      8.0\n",
       "Neurology      10.0\n",
       "Orthopedics     8.7\n",
       "Pediatrics      6.0\n",
       "Name: HospitalStayDays, dtype: float64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data['AdmissionDate'] = pd.to_datetime(data['AdmissionDate'])\n",
    "data.groupby('Department')['HospitalStayDays'].mean().round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2a4491",
   "metadata": {},
   "source": [
    "• Identify the patient with the longest hospital stay.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "fa4d64c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bob Smith'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[data['HospitalStayDays'].idxmax()]['PatientName']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edad12f4",
   "metadata": {},
   "source": [
    "• Calculate the number of patients admitted per month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f028e23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdmissionDate\n",
       "2025-01    2\n",
       "2025-02    1\n",
       "2025-03    2\n",
       "2025-04    2\n",
       "2025-05    2\n",
       "2025-06    2\n",
       "2025-07    1\n",
       "Freq: M, Name: PatientID, dtype: int64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(data['AdmissionDate'].dt.to_period('M'))['PatientID'].count()####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e58ec75",
   "metadata": {},
   "source": [
    "---\n",
    "**6. Banking Transactions Analysis Use Case**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d193279b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>AccountID</th>\n",
       "      <th>CustomerName</th>\n",
       "      <th>TransactionType</th>\n",
       "      <th>TransactionAmount</th>\n",
       "      <th>TransactionDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T001</td>\n",
       "      <td>A001</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Deposit</td>\n",
       "      <td>1200</td>\n",
       "      <td>2025-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T002</td>\n",
       "      <td>A002</td>\n",
       "      <td>Bob</td>\n",
       "      <td>Withdrawal</td>\n",
       "      <td>500</td>\n",
       "      <td>2025-01-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T003</td>\n",
       "      <td>A001</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Deposit</td>\n",
       "      <td>1500</td>\n",
       "      <td>2025-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T004</td>\n",
       "      <td>A003</td>\n",
       "      <td>Carol</td>\n",
       "      <td>Deposit</td>\n",
       "      <td>2000</td>\n",
       "      <td>2025-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T005</td>\n",
       "      <td>A002</td>\n",
       "      <td>Bob</td>\n",
       "      <td>Withdrawal</td>\n",
       "      <td>700</td>\n",
       "      <td>2025-02-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T006</td>\n",
       "      <td>A001</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Withdrawal</td>\n",
       "      <td>600</td>\n",
       "      <td>2025-02-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>T007</td>\n",
       "      <td>A004</td>\n",
       "      <td>David</td>\n",
       "      <td>Deposit</td>\n",
       "      <td>3000</td>\n",
       "      <td>2025-03-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T008</td>\n",
       "      <td>A003</td>\n",
       "      <td>Carol</td>\n",
       "      <td>Withdrawal</td>\n",
       "      <td>1000</td>\n",
       "      <td>2025-03-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>T009</td>\n",
       "      <td>A004</td>\n",
       "      <td>David</td>\n",
       "      <td>Deposit</td>\n",
       "      <td>2500</td>\n",
       "      <td>2025-03-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>T010</td>\n",
       "      <td>A002</td>\n",
       "      <td>Bob</td>\n",
       "      <td>Deposit</td>\n",
       "      <td>1200</td>\n",
       "      <td>2025-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>T011</td>\n",
       "      <td>A003</td>\n",
       "      <td>Carol</td>\n",
       "      <td>Withdrawal</td>\n",
       "      <td>800</td>\n",
       "      <td>2025-04-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>T012</td>\n",
       "      <td>A001</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Deposit</td>\n",
       "      <td>1800</td>\n",
       "      <td>2025-04-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID AccountID CustomerName TransactionType  TransactionAmount  \\\n",
       "0           T001      A001        Alice         Deposit               1200   \n",
       "1           T002      A002          Bob      Withdrawal                500   \n",
       "2           T003      A001        Alice         Deposit               1500   \n",
       "3           T004      A003        Carol         Deposit               2000   \n",
       "4           T005      A002          Bob      Withdrawal                700   \n",
       "5           T006      A001        Alice      Withdrawal                600   \n",
       "6           T007      A004        David         Deposit               3000   \n",
       "7           T008      A003        Carol      Withdrawal               1000   \n",
       "8           T009      A004        David         Deposit               2500   \n",
       "9           T010      A002          Bob         Deposit               1200   \n",
       "10          T011      A003        Carol      Withdrawal                800   \n",
       "11          T012      A001        Alice         Deposit               1800   \n",
       "\n",
       "   TransactionDate  \n",
       "0       2025-01-05  \n",
       "1       2025-01-07  \n",
       "2       2025-01-10  \n",
       "3       2025-02-01  \n",
       "4       2025-02-05  \n",
       "5       2025-02-10  \n",
       "6       2025-03-03  \n",
       "7       2025-03-10  \n",
       "8       2025-03-15  \n",
       "9       2025-04-01  \n",
       "10      2025-04-05  \n",
       "11      2025-04-10  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_transactions_dict = {\n",
    "    \"TransactionID\": ['T001', 'T002', 'T003', 'T004', 'T005', 'T006', 'T007', 'T008', 'T009', 'T010', 'T011', 'T012'],\n",
    "    \"AccountID\": ['A001', 'A002', 'A001', 'A003', 'A002', 'A001', 'A004', 'A003', 'A004', 'A002', 'A003', 'A001'],\n",
    "    \"CustomerName\": ['Alice', 'Bob', 'Alice', 'Carol', 'Bob', 'Alice', 'David', 'Carol', 'David', 'Bob', 'Carol', 'Alice'],\n",
    "    \"TransactionType\": ['Deposit', 'Withdrawal', 'Deposit', 'Deposit', 'Withdrawal', 'Withdrawal', 'Deposit', 'Withdrawal', 'Deposit', 'Deposit', 'Withdrawal', 'Deposit'],\n",
    "    \"TransactionAmount\": [1200, 500, 1500, 2000, 700, 600, 3000, 1000, 2500, 1200, 800, 1800],\n",
    "    \"TransactionDate\": ['2025-01-05', '2025-01-07', '2025-01-10', '2025-02-01', '2025-02-05', '2025-02-10',\n",
    "                        '2025-03-03', '2025-03-10', '2025-03-15', '2025-04-01', '2025-04-05', '2025-04-10']\n",
    "}\n",
    "data = pd.DataFrame(bank_transactions_dict)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f1f1ec",
   "metadata": {},
   "source": [
    "• Count the total number of transactions per account.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "bfd8ef1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AccountID\n",
       "A001    4\n",
       "A002    3\n",
       "A003    3\n",
       "A004    2\n",
       "Name: TransactionID, dtype: int64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('AccountID')['TransactionID'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f05f7b",
   "metadata": {},
   "source": [
    "• Find the account with the highest transaction value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d30dea62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransactionID              T007\n",
       "AccountID                  A004\n",
       "CustomerName              David\n",
       "TransactionType         Deposit\n",
       "TransactionAmount          3000\n",
       "TransactionDate      2025-03-03\n",
       "Name: 6, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[data['TransactionAmount'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7293bb",
   "metadata": {},
   "source": [
    "• Calculate average transaction amount per customer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e96011f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AccountID\n",
       "A001    1275.000000\n",
       "A002     800.000000\n",
       "A003    1266.666667\n",
       "A004    2750.000000\n",
       "Name: TransactionAmount, dtype: float64"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('AccountID')['TransactionAmount'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17456535",
   "metadata": {},
   "source": [
    "• Group transactions by type (deposit, withdrawal) and calculate totals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5f536578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransactionType\n",
       "Deposit       13200\n",
       "Withdrawal     3600\n",
       "Name: TransactionAmount, dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('TransactionType')['TransactionAmount'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983ce3c9",
   "metadata": {},
   "source": [
    "---\n",
    "**7. Retail Store Inventory Analysis Use Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1aa5cb68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductID</th>\n",
       "      <th>Category</th>\n",
       "      <th>ProductName</th>\n",
       "      <th>Stock</th>\n",
       "      <th>ReorderLevel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P001</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>Cola 500ml</td>\n",
       "      <td>120</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P002</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>Orange Juice 1L</td>\n",
       "      <td>45</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P003</td>\n",
       "      <td>Snacks</td>\n",
       "      <td>Potato Chips 200g</td>\n",
       "      <td>80</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P004</td>\n",
       "      <td>Snacks</td>\n",
       "      <td>Chocolate Bar 50g</td>\n",
       "      <td>25</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P005</td>\n",
       "      <td>Dairy</td>\n",
       "      <td>Milk 1L</td>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P006</td>\n",
       "      <td>Dairy</td>\n",
       "      <td>Cheddar Cheese 200g</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P007</td>\n",
       "      <td>Bakery</td>\n",
       "      <td>Whole Wheat Bread</td>\n",
       "      <td>35</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P008</td>\n",
       "      <td>Bakery</td>\n",
       "      <td>Croissant Pack (4)</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P009</td>\n",
       "      <td>Household</td>\n",
       "      <td>Laundry Detergent 2kg</td>\n",
       "      <td>90</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>P010</td>\n",
       "      <td>Household</td>\n",
       "      <td>Dish Soap 500ml</td>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ProductID   Category            ProductName  Stock  ReorderLevel\n",
       "0      P001  Beverages             Cola 500ml    120            50\n",
       "1      P002  Beverages        Orange Juice 1L     45            60\n",
       "2      P003     Snacks      Potato Chips 200g     80            40\n",
       "3      P004     Snacks      Chocolate Bar 50g     25            30\n",
       "4      P005      Dairy                Milk 1L     60            40\n",
       "5      P006      Dairy    Cheddar Cheese 200g     15            20\n",
       "6      P007     Bakery      Whole Wheat Bread     35            25\n",
       "7      P008     Bakery     Croissant Pack (4)     10            15\n",
       "8      P009  Household  Laundry Detergent 2kg     90            30\n",
       "9      P010  Household        Dish Soap 500ml     50            20"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inventory_dict = {\n",
    "    \"ProductID\": ['P001','P002','P003','P004','P005','P006','P007','P008','P009','P010'],\n",
    "    \"Category\": ['Beverages','Beverages','Snacks','Snacks','Dairy','Dairy','Bakery','Bakery','Household','Household'],\n",
    "    \"ProductName\": [\n",
    "        'Cola 500ml', 'Orange Juice 1L', 'Potato Chips 200g', 'Chocolate Bar 50g',\n",
    "        'Milk 1L', 'Cheddar Cheese 200g', 'Whole Wheat Bread', 'Croissant Pack (4)',\n",
    "        'Laundry Detergent 2kg', 'Dish Soap 500ml'\n",
    "    ],\n",
    "    \"Stock\": [120, 45, 80, 25, 60, 15, 35, 10, 90, 50],\n",
    "    \"ReorderLevel\": [50, 60, 40, 30, 40, 20, 25, 15, 30, 20]\n",
    "}\n",
    "data = pd.DataFrame(inventory_dict)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f429709c",
   "metadata": {},
   "source": [
    "• Count the number of products in each category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bae5e336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "Bakery       2\n",
       "Beverages    2\n",
       "Dairy        2\n",
       "Household    2\n",
       "Snacks       2\n",
       "Name: ProductID, dtype: int64"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('Category')['ProductID'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07253f55",
   "metadata": {},
   "source": [
    "• Find the product with the lowest stock quantity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "432b4e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Croissant Pack (4)'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[data['Stock'].idxmin()]['ProductName']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d845f02",
   "metadata": {},
   "source": [
    "• Calculate the average stock value per category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "56e0be01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "Bakery       22.5\n",
       "Beverages    82.5\n",
       "Dairy        37.5\n",
       "Household    70.0\n",
       "Snacks       52.5\n",
       "Name: Stock, dtype: float64"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('Category')[\"Stock\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d42d5",
   "metadata": {},
   "source": [
    "• Identify products that need restocking (below threshold = 60).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6db77719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ProductID Category          ProductName  Stock  ReorderLevel\n",
      "3      P004   Snacks    Chocolate Bar 50g     25            30\n",
      "5      P006    Dairy  Cheddar Cheese 200g     15            20\n",
      "7      P008   Bakery   Croissant Pack (4)     10            15\n"
     ]
    }
   ],
   "source": [
    "print(data[(data['Stock']<30)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cdd011",
   "metadata": {},
   "source": [
    "---\n",
    "**8. Social Media Engagement Analysis Use Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "cb846249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PostID</th>\n",
       "      <th>UserID</th>\n",
       "      <th>UserName</th>\n",
       "      <th>PostContent</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Comments</th>\n",
       "      <th>PostDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P001</td>\n",
       "      <td>U001</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Had a great day!</td>\n",
       "      <td>120</td>\n",
       "      <td>15</td>\n",
       "      <td>2025-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P002</td>\n",
       "      <td>U002</td>\n",
       "      <td>Bob</td>\n",
       "      <td>Check out this photo!</td>\n",
       "      <td>85</td>\n",
       "      <td>10</td>\n",
       "      <td>2025-08-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P003</td>\n",
       "      <td>U001</td>\n",
       "      <td>Alice</td>\n",
       "      <td>New blog post is up</td>\n",
       "      <td>150</td>\n",
       "      <td>25</td>\n",
       "      <td>2025-08-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P004</td>\n",
       "      <td>U003</td>\n",
       "      <td>Carol</td>\n",
       "      <td>Lovely sunset!</td>\n",
       "      <td>200</td>\n",
       "      <td>30</td>\n",
       "      <td>2025-08-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P005</td>\n",
       "      <td>U002</td>\n",
       "      <td>Bob</td>\n",
       "      <td>Weekend vibes</td>\n",
       "      <td>90</td>\n",
       "      <td>8</td>\n",
       "      <td>2025-08-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P006</td>\n",
       "      <td>U001</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Sharing a tip</td>\n",
       "      <td>130</td>\n",
       "      <td>20</td>\n",
       "      <td>2025-08-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P007</td>\n",
       "      <td>U004</td>\n",
       "      <td>David</td>\n",
       "      <td>Throwback Thursday</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-08-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P008</td>\n",
       "      <td>U003</td>\n",
       "      <td>Carol</td>\n",
       "      <td>Morning coffee</td>\n",
       "      <td>110</td>\n",
       "      <td>12</td>\n",
       "      <td>2025-08-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P009</td>\n",
       "      <td>U004</td>\n",
       "      <td>David</td>\n",
       "      <td>Fitness update</td>\n",
       "      <td>75</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-08-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>P010</td>\n",
       "      <td>U001</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Book recommendation</td>\n",
       "      <td>160</td>\n",
       "      <td>18</td>\n",
       "      <td>2025-08-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>P011</td>\n",
       "      <td>U002</td>\n",
       "      <td>Bob</td>\n",
       "      <td>Holiday photos</td>\n",
       "      <td>95</td>\n",
       "      <td>9</td>\n",
       "      <td>2025-08-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>P012</td>\n",
       "      <td>U003</td>\n",
       "      <td>Carol</td>\n",
       "      <td>Movie review</td>\n",
       "      <td>180</td>\n",
       "      <td>22</td>\n",
       "      <td>2025-08-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PostID UserID UserName            PostContent  Likes  Comments    PostDate\n",
       "0    P001   U001    Alice       Had a great day!    120        15  2025-08-01\n",
       "1    P002   U002      Bob  Check out this photo!     85        10  2025-08-02\n",
       "2    P003   U001    Alice    New blog post is up    150        25  2025-08-03\n",
       "3    P004   U003    Carol         Lovely sunset!    200        30  2025-08-04\n",
       "4    P005   U002      Bob          Weekend vibes     90         8  2025-08-05\n",
       "5    P006   U001    Alice          Sharing a tip    130        20  2025-08-06\n",
       "6    P007   U004    David     Throwback Thursday     50         5  2025-08-07\n",
       "7    P008   U003    Carol         Morning coffee    110        12  2025-08-08\n",
       "8    P009   U004    David         Fitness update     75         4  2025-08-09\n",
       "9    P010   U001    Alice    Book recommendation    160        18  2025-08-10\n",
       "10   P011   U002      Bob         Holiday photos     95         9  2025-08-11\n",
       "11   P012   U003    Carol           Movie review    180        22  2025-08-12"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "social_media_data = {\n",
    "    \"PostID\": ['P001','P002','P003','P004','P005','P006','P007','P008','P009','P010','P011','P012'],\n",
    "    \"UserID\": ['U001','U002','U001','U003','U002','U001','U004','U003','U004','U001','U002','U003'],\n",
    "    \"UserName\": ['Alice','Bob','Alice','Carol','Bob','Alice','David','Carol','David','Alice','Bob','Carol'],\n",
    "    \"PostContent\": [\n",
    "        'Had a great day!', 'Check out this photo!', 'New blog post is up', 'Lovely sunset!',\n",
    "        'Weekend vibes', 'Sharing a tip', 'Throwback Thursday', 'Morning coffee', 'Fitness update',\n",
    "        'Book recommendation', 'Holiday photos', 'Movie review'\n",
    "    ],\n",
    "    \"Likes\": [120, 85, 150, 200, 90, 130, 50, 110, 75, 160, 95, 180],\n",
    "    \"Comments\": [15, 10, 25, 30, 8, 20, 5, 12, 4, 18, 9, 22],\n",
    "    \"PostDate\": [\n",
    "        '2025-08-01','2025-08-02','2025-08-03','2025-08-04','2025-08-05','2025-08-06',\n",
    "        '2025-08-07','2025-08-08','2025-08-09','2025-08-10','2025-08-11','2025-08-12'\n",
    "    ]\n",
    "}\n",
    "\n",
    "data = pd.DataFrame(social_media_data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c72044",
   "metadata": {},
   "source": [
    "• Count the number of posts per user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "09802da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserID\n",
       "U001    4\n",
       "U002    3\n",
       "U003    3\n",
       "U004    2\n",
       "Name: PostID, dtype: int64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('UserID')['PostID'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fe6216",
   "metadata": {},
   "source": [
    "• Find the post with the highest number of likes or comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c96e46a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Carol'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[data['Likes'].idxmax()]['UserName']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9f0c50",
   "metadata": {},
   "source": [
    "• Calculate average likes per user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "21ae3f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserID\n",
       "U001    140.000000\n",
       "U002     90.000000\n",
       "U003    163.333333\n",
       "U004     62.500000\n",
       "Name: Likes, dtype: float64"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('UserID')['Likes'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6df9db7",
   "metadata": {},
   "source": [
    "• Identify the most active users based on posting frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5a44fc01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'U001'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('UserID')['PostID'].count().idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd35a3c",
   "metadata": {},
   "source": [
    "---\n",
    "**9. Airline Flight Data Analysis Use Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f3cf3584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FlightID</th>\n",
       "      <th>Airline</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>DepartureTime</th>\n",
       "      <th>ArrivalTime</th>\n",
       "      <th>FlightDurationMinutes</th>\n",
       "      <th>DelayMinutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F001</td>\n",
       "      <td>AirAlpha</td>\n",
       "      <td>New York</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>2025-08-01 08:00</td>\n",
       "      <td>2025-08-01 11:00</td>\n",
       "      <td>180</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F002</td>\n",
       "      <td>AirBeta</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>New York</td>\n",
       "      <td>2025-08-01 09:00</td>\n",
       "      <td>2025-08-01 13:00</td>\n",
       "      <td>240</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F003</td>\n",
       "      <td>AirAlpha</td>\n",
       "      <td>New York</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>2025-08-02 10:00</td>\n",
       "      <td>2025-08-02 13:00</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F004</td>\n",
       "      <td>AirGamma</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>New York</td>\n",
       "      <td>2025-08-02 12:00</td>\n",
       "      <td>2025-08-02 15:30</td>\n",
       "      <td>210</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F005</td>\n",
       "      <td>AirBeta</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>2025-08-03 14:00</td>\n",
       "      <td>2025-08-03 17:00</td>\n",
       "      <td>180</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F006</td>\n",
       "      <td>AirAlpha</td>\n",
       "      <td>New York</td>\n",
       "      <td>Miami</td>\n",
       "      <td>2025-08-03 16:00</td>\n",
       "      <td>2025-08-03 19:30</td>\n",
       "      <td>210</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F007</td>\n",
       "      <td>AirGamma</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Miami</td>\n",
       "      <td>2025-08-04 08:00</td>\n",
       "      <td>2025-08-04 11:30</td>\n",
       "      <td>210</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F008</td>\n",
       "      <td>AirBeta</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Miami</td>\n",
       "      <td>2025-08-04 09:00</td>\n",
       "      <td>2025-08-04 12:45</td>\n",
       "      <td>225</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F009</td>\n",
       "      <td>AirAlpha</td>\n",
       "      <td>New York</td>\n",
       "      <td>Miami</td>\n",
       "      <td>2025-08-05 10:00</td>\n",
       "      <td>2025-08-05 13:00</td>\n",
       "      <td>180</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F010</td>\n",
       "      <td>AirGamma</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Miami</td>\n",
       "      <td>2025-08-05 12:00</td>\n",
       "      <td>2025-08-05 15:30</td>\n",
       "      <td>210</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F011</td>\n",
       "      <td>AirBeta</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Miami</td>\n",
       "      <td>2025-08-06 14:00</td>\n",
       "      <td>2025-08-06 17:30</td>\n",
       "      <td>210</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F012</td>\n",
       "      <td>AirAlpha</td>\n",
       "      <td>New York</td>\n",
       "      <td>Miami</td>\n",
       "      <td>2025-08-06 16:00</td>\n",
       "      <td>2025-08-06 18:45</td>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FlightID   Airline       Origin Destination     DepartureTime  \\\n",
       "0      F001  AirAlpha     New York     Chicago  2025-08-01 08:00   \n",
       "1      F002   AirBeta  Los Angeles    New York  2025-08-01 09:00   \n",
       "2      F003  AirAlpha     New York     Chicago  2025-08-02 10:00   \n",
       "3      F004  AirGamma      Chicago    New York  2025-08-02 12:00   \n",
       "4      F005   AirBeta  Los Angeles     Chicago  2025-08-03 14:00   \n",
       "5      F006  AirAlpha     New York       Miami  2025-08-03 16:00   \n",
       "6      F007  AirGamma      Chicago       Miami  2025-08-04 08:00   \n",
       "7      F008   AirBeta  Los Angeles       Miami  2025-08-04 09:00   \n",
       "8      F009  AirAlpha     New York       Miami  2025-08-05 10:00   \n",
       "9      F010  AirGamma      Chicago       Miami  2025-08-05 12:00   \n",
       "10     F011   AirBeta  Los Angeles       Miami  2025-08-06 14:00   \n",
       "11     F012  AirAlpha     New York       Miami  2025-08-06 16:00   \n",
       "\n",
       "         ArrivalTime  FlightDurationMinutes  DelayMinutes  \n",
       "0   2025-08-01 11:00                    180            15  \n",
       "1   2025-08-01 13:00                    240            30  \n",
       "2   2025-08-02 13:00                    180             0  \n",
       "3   2025-08-02 15:30                    210            10  \n",
       "4   2025-08-03 17:00                    180             5  \n",
       "5   2025-08-03 19:30                    210            20  \n",
       "6   2025-08-04 11:30                    210            15  \n",
       "7   2025-08-04 12:45                    225             0  \n",
       "8   2025-08-05 13:00                    180            25  \n",
       "9   2025-08-05 15:30                    210             5  \n",
       "10  2025-08-06 17:30                    210            10  \n",
       "11  2025-08-06 18:45                    165             0  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_data = {\n",
    "    \"FlightID\": ['F001','F002','F003','F004','F005','F006','F007','F008','F009','F010','F011','F012'],\n",
    "    \"Airline\": ['AirAlpha','AirBeta','AirAlpha','AirGamma','AirBeta','AirAlpha','AirGamma','AirBeta','AirAlpha','AirGamma','AirBeta','AirAlpha'],\n",
    "    \"Origin\": ['New York','Los Angeles','New York','Chicago','Los Angeles','New York','Chicago','Los Angeles','New York','Chicago','Los Angeles','New York'],\n",
    "    \"Destination\": ['Chicago','New York','Chicago','New York','Chicago','Miami','Miami','Miami','Miami','Miami','Miami','Miami'],\n",
    "    \"DepartureTime\": ['2025-08-01 08:00','2025-08-01 09:00','2025-08-02 10:00','2025-08-02 12:00',\n",
    "                      '2025-08-03 14:00','2025-08-03 16:00','2025-08-04 08:00','2025-08-04 09:00',\n",
    "                      '2025-08-05 10:00','2025-08-05 12:00','2025-08-06 14:00','2025-08-06 16:00'],\n",
    "    \"ArrivalTime\": ['2025-08-01 11:00','2025-08-01 13:00','2025-08-02 13:00','2025-08-02 15:30',\n",
    "                    '2025-08-03 17:00','2025-08-03 19:30','2025-08-04 11:30','2025-08-04 12:45',\n",
    "                    '2025-08-05 13:00','2025-08-05 15:30','2025-08-06 17:30','2025-08-06 18:45'],\n",
    "    \"FlightDurationMinutes\": [180, 240, 180, 210, 180, 210, 210, 225, 180, 210, 210, 165],\n",
    "    \"DelayMinutes\": [15, 30, 0, 10, 5, 20, 15, 0, 25, 5, 10, 0]\n",
    "}\n",
    "data = pd.DataFrame(flight_data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eaac92",
   "metadata": {},
   "source": [
    "• Count the number of flights per airline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d3f5cf8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Airline\n",
       "AirAlpha    5\n",
       "AirBeta     4\n",
       "AirGamma    3\n",
       "Name: FlightID, dtype: int64"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('Airline')['FlightID'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c94168d",
   "metadata": {},
   "source": [
    "• Calculate average flight delay per airline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7bc9b0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Airline\n",
       "AirAlpha    12.00\n",
       "AirBeta     11.25\n",
       "AirGamma    10.00\n",
       "Name: DelayMinutes, dtype: float64"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('Airline')['DelayMinutes'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf6c239",
   "metadata": {},
   "source": [
    "• Identify the route with the maximum number of flights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "80337a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Miami'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(\"Destination\")['FlightID'].count().idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d61a4b",
   "metadata": {},
   "source": [
    "• Find the longest and shortest flight duration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8038ab25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York\n",
      "Miami\n"
     ]
    }
   ],
   "source": [
    "print(data.iloc[data['FlightDurationMinutes'].idxmax()]['Destination'])\n",
    "print(data.iloc[data['FlightDurationMinutes'].idxmin()]['Destination'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb33ffb1",
   "metadata": {},
   "source": [
    "---\n",
    "**10. Movie Ratings Analysis Use Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1106963c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Director</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M001</td>\n",
       "      <td>The Adventure</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>John Smith</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M001</td>\n",
       "      <td>The Adventure</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>John Smith</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M002</td>\n",
       "      <td>Space Journey</td>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>Alice Brown</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M002</td>\n",
       "      <td>Space Journey</td>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>Alice Brown</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M003</td>\n",
       "      <td>Mystery Night</td>\n",
       "      <td>Mystery</td>\n",
       "      <td>Carol Lee</td>\n",
       "      <td>7.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M003</td>\n",
       "      <td>Mystery Night</td>\n",
       "      <td>Mystery</td>\n",
       "      <td>Carol Lee</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M004</td>\n",
       "      <td>Love in Paris</td>\n",
       "      <td>Romance</td>\n",
       "      <td>David Kim</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M004</td>\n",
       "      <td>Love in Paris</td>\n",
       "      <td>Romance</td>\n",
       "      <td>David Kim</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M005</td>\n",
       "      <td>Tech Revolution</td>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>Alice Brown</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M005</td>\n",
       "      <td>Tech Revolution</td>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>Alice Brown</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M006</td>\n",
       "      <td>Hidden Secrets</td>\n",
       "      <td>Mystery</td>\n",
       "      <td>Carol Lee</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M006</td>\n",
       "      <td>Hidden Secrets</td>\n",
       "      <td>Mystery</td>\n",
       "      <td>Carol Lee</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M007</td>\n",
       "      <td>Comedy Club</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Emma Davis</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M007</td>\n",
       "      <td>Comedy Club</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Emma Davis</td>\n",
       "      <td>7.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M008</td>\n",
       "      <td>Horror House</td>\n",
       "      <td>Horror</td>\n",
       "      <td>Frank Wilson</td>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M008</td>\n",
       "      <td>Horror House</td>\n",
       "      <td>Horror</td>\n",
       "      <td>Frank Wilson</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M009</td>\n",
       "      <td>Drama Tales</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Grace Chen</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M009</td>\n",
       "      <td>Drama Tales</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Grace Chen</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M010</td>\n",
       "      <td>Fantasy World</td>\n",
       "      <td>Fantasy</td>\n",
       "      <td>Henry Patel</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M010</td>\n",
       "      <td>Fantasy World</td>\n",
       "      <td>Fantasy</td>\n",
       "      <td>Henry Patel</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M011</td>\n",
       "      <td>Sci-Fi Saga</td>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>Alice Brown</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M011</td>\n",
       "      <td>Sci-Fi Saga</td>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>Alice Brown</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>M012</td>\n",
       "      <td>Romantic Escape</td>\n",
       "      <td>Romance</td>\n",
       "      <td>David Kim</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>M012</td>\n",
       "      <td>Romantic Escape</td>\n",
       "      <td>Romance</td>\n",
       "      <td>David Kim</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>M013</td>\n",
       "      <td>Thriller Night</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>Carol Lee</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>M013</td>\n",
       "      <td>Thriller Night</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>Carol Lee</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>M014</td>\n",
       "      <td>Action Blast</td>\n",
       "      <td>Action</td>\n",
       "      <td>Emma Davis</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>M014</td>\n",
       "      <td>Action Blast</td>\n",
       "      <td>Action</td>\n",
       "      <td>Emma Davis</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>M015</td>\n",
       "      <td>Family Fun</td>\n",
       "      <td>Family</td>\n",
       "      <td>Grace Chen</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>M015</td>\n",
       "      <td>Family Fun</td>\n",
       "      <td>Family</td>\n",
       "      <td>Grace Chen</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID            Title      Genre      Director  Rating\n",
       "0     M001    The Adventure  Adventure    John Smith     8.5\n",
       "1     M001    The Adventure  Adventure    John Smith     8.8\n",
       "2     M002    Space Journey     Sci-Fi   Alice Brown     9.2\n",
       "3     M002    Space Journey     Sci-Fi   Alice Brown     8.9\n",
       "4     M003    Mystery Night    Mystery     Carol Lee     7.8\n",
       "5     M003    Mystery Night    Mystery     Carol Lee     8.0\n",
       "6     M004    Love in Paris    Romance     David Kim     8.0\n",
       "7     M004    Love in Paris    Romance     David Kim     7.9\n",
       "8     M005  Tech Revolution     Sci-Fi   Alice Brown     9.0\n",
       "9     M005  Tech Revolution     Sci-Fi   Alice Brown     9.1\n",
       "10    M006   Hidden Secrets    Mystery     Carol Lee     8.1\n",
       "11    M006   Hidden Secrets    Mystery     Carol Lee     7.9\n",
       "12    M007      Comedy Club     Comedy    Emma Davis     7.5\n",
       "13    M007      Comedy Club     Comedy    Emma Davis     7.8\n",
       "14    M008     Horror House     Horror  Frank Wilson     6.8\n",
       "15    M008     Horror House     Horror  Frank Wilson     7.0\n",
       "16    M009      Drama Tales      Drama    Grace Chen     8.3\n",
       "17    M009      Drama Tales      Drama    Grace Chen     8.4\n",
       "18    M010    Fantasy World    Fantasy   Henry Patel     9.1\n",
       "19    M010    Fantasy World    Fantasy   Henry Patel     9.0\n",
       "20    M011      Sci-Fi Saga     Sci-Fi   Alice Brown     9.3\n",
       "21    M011      Sci-Fi Saga     Sci-Fi   Alice Brown     9.0\n",
       "22    M012  Romantic Escape    Romance     David Kim     8.2\n",
       "23    M012  Romantic Escape    Romance     David Kim     8.1\n",
       "24    M013   Thriller Night   Thriller     Carol Lee     7.7\n",
       "25    M013   Thriller Night   Thriller     Carol Lee     8.0\n",
       "26    M014     Action Blast     Action    Emma Davis     8.5\n",
       "27    M014     Action Blast     Action    Emma Davis     8.3\n",
       "28    M015       Family Fun     Family    Grace Chen     7.9\n",
       "29    M015       Family Fun     Family    Grace Chen     8.1"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "movie_ratings_data = {\n",
    "    \"MovieID\": [\n",
    "        'M001','M001','M002','M002','M003','M003','M004','M004','M005','M005',\n",
    "        'M006','M006','M007','M007','M008','M008','M009','M009','M010','M010',\n",
    "        'M011','M011','M012','M012','M013','M013','M014','M014','M015','M015'\n",
    "    ],\n",
    "    \"Title\": [\n",
    "        'The Adventure','The Adventure','Space Journey','Space Journey','Mystery Night','Mystery Night',\n",
    "        'Love in Paris','Love in Paris','Tech Revolution','Tech Revolution','Hidden Secrets','Hidden Secrets',\n",
    "        'Comedy Club','Comedy Club','Horror House','Horror House','Drama Tales','Drama Tales','Fantasy World','Fantasy World',\n",
    "        'Sci-Fi Saga','Sci-Fi Saga','Romantic Escape','Romantic Escape','Thriller Night','Thriller Night','Action Blast','Action Blast','Family Fun','Family Fun'\n",
    "    ],\n",
    "    \"Genre\": [\n",
    "        'Adventure','Adventure','Sci-Fi','Sci-Fi','Mystery','Mystery','Romance','Romance','Sci-Fi','Sci-Fi',\n",
    "        'Mystery','Mystery','Comedy','Comedy','Horror','Horror','Drama','Drama','Fantasy','Fantasy',\n",
    "        'Sci-Fi','Sci-Fi','Romance','Romance','Thriller','Thriller','Action','Action','Family','Family'\n",
    "    ],\n",
    "    \"Director\": [\n",
    "        'John Smith','John Smith','Alice Brown','Alice Brown','Carol Lee','Carol Lee','David Kim','David Kim',\n",
    "        'Alice Brown','Alice Brown','Carol Lee','Carol Lee','Emma Davis','Emma Davis','Frank Wilson','Frank Wilson',\n",
    "        'Grace Chen','Grace Chen','Henry Patel','Henry Patel','Alice Brown','Alice Brown','David Kim','David Kim',\n",
    "        'Carol Lee','Carol Lee','Emma Davis','Emma Davis','Grace Chen','Grace Chen'\n",
    "    ],\n",
    "    \"Rating\": [\n",
    "        8.5, 8.8, 9.2, 8.9, 7.8, 8.0, 8.0, 7.9, 9.0, 9.1, 8.1, 7.9, 7.5, 7.8, 6.8, 7.0, 8.3, 8.4, 9.1, 9.0,\n",
    "        9.3, 9.0, 8.2, 8.1, 7.7, 8.0, 8.5, 8.3, 7.9, 8.1\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "data = pd.DataFrame(movie_ratings_data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235640cc",
   "metadata": {},
   "source": [
    "• Count the number of movies per genre.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c7c73fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Genre\n",
       "Action       2\n",
       "Adventure    2\n",
       "Comedy       2\n",
       "Drama        2\n",
       "Family       2\n",
       "Fantasy      2\n",
       "Horror       2\n",
       "Mystery      4\n",
       "Romance      4\n",
       "Sci-Fi       6\n",
       "Thriller     2\n",
       "Name: MovieID, dtype: int64"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('Genre')['MovieID'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf38b03",
   "metadata": {},
   "source": [
    "• Calculate the average rating per movie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e75a7fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MovieID\n",
       "M001    8.65\n",
       "M002    9.05\n",
       "M003    7.90\n",
       "M004    7.95\n",
       "M005    9.05\n",
       "M006    8.00\n",
       "M007    7.65\n",
       "M008    6.90\n",
       "M009    8.35\n",
       "M010    9.05\n",
       "M011    9.15\n",
       "M012    8.15\n",
       "M013    7.85\n",
       "M014    8.40\n",
       "M015    8.00\n",
       "Name: Rating, dtype: float64"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('MovieID')['Rating'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203c34a8",
   "metadata": {},
   "source": [
    "• Find the movie with the highest rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "47df4e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sci-Fi Saga'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[data['Rating'].idxmax()]['Title']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d379d38c",
   "metadata": {},
   "source": [
    "• Identify top 5 directors based on average movie ratings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "913ad67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Director\n",
       "Alice Brown     9.083333\n",
       "Carol Lee       7.916667\n",
       "David Kim       8.050000\n",
       "Emma Davis      8.025000\n",
       "Frank Wilson    6.900000\n",
       "Grace Chen      8.175000\n",
       "Henry Patel     9.050000\n",
       "John Smith      8.650000\n",
       "Name: Rating, dtype: float64"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(\"Director\")['Rating'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8edd5b",
   "metadata": {},
   "source": [
    "---\n",
    "**Pandas Treasure Hunt: The Hidden Wealth of Vijayanagara**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2ced83",
   "metadata": {},
   "source": [
    "It is the 16th century. The Vijayanagara Empire thrives, with bustling bazaars, grand gopurams, and far-reaching trade in spices, silk, and gems. Rumors whisper that King Krishnadevaraya hid a treasure of knowledge and wealth in secret chambers beneath Hampi. The treasure can only be revealed by solving 20 data scrolls preserved by scholars, merchants, and temple priests. You are a young data explorer chosen to decode these scrolls using pandas, traveling across the empire, solving puzzles, and uncovering secrets. Each solved scroll brings you closer to the treasure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a82ab970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series:\n",
      " Sangama     5000\n",
      "Saluva      4200\n",
      "Tuluva      4800\n",
      "Aravindu    5100\n",
      "Vijaya      4700\n",
      "Rama        4500\n",
      "Hari        4200\n",
      "Deva        4600\n",
      "Vira        4900\n",
      "Chandra     4400\n",
      "Name: Tribute, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "families = [\"Sangama\", \"Saluva\", \"Tuluva\", \"Aravindu\",\"Vijaya\",\"Rama\",\"Hari\",\"Deva\",\"Vira\", \"Chandra\"]\n",
    "tributes = [5000, 4200, 4800, 5100, 4700,4500,4200,4600,4900,4400]\n",
    "tribute_series = pd.Series(data=tributes, index=families, name=\"Tribute\")\n",
    "print(\"Series:\\n\", tribute_series, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d120762",
   "metadata": {},
   "source": [
    "Scene 1:\n",
    "\n",
    " The Royal Decree of Hampi As you enter the palace, the royal librarian hands you a scroll listing noble families and their tribute. Some names are written in old Kannada script, and some tribute values seem oddly high.\n",
    "\n",
    "Task: Create a Series and DataFrame with family names and tribute amounts.\n",
    "\n",
    " Use the family names as the index. (Concept: Series/DataFrame creation + custom index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f58944e",
   "metadata": {},
   "source": [
    "Scene 2:\n",
    "\n",
    " The Hampi Bazaar The largest market dazzles with diamonds, horses, spices, and silk. Merchants shout their prices, and you need to find only the stalls selling precious items.\n",
    "\n",
    " Task: Slice the DataFrame to show only the first 10 rows and last 5 rows. (Concept: Indexing & slicing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "c32cfaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 stalls:\n",
      "       Merchant     Item  Price\n",
      "0   Merchant_1     Gold   2970\n",
      "1   Merchant_2  Diamond  17552\n",
      "2   Merchant_3     Gold  30984\n",
      "3   Merchant_4    Horse  47965\n",
      "4   Merchant_5   Spices  26610\n",
      "5   Merchant_6     Gold  12397\n",
      "6   Merchant_7  Diamond  38861\n",
      "7   Merchant_8     Silk  40106\n",
      "8   Merchant_9     Gold  29126\n",
      "9  Merchant_10  Diamond  25728 \n",
      "\n",
      "Last 5 stalls:\n",
      "        Merchant     Item  Price\n",
      "25  Merchant_26   Spices  49043\n",
      "26  Merchant_27    Horse  20236\n",
      "27  Merchant_28     Gold  27408\n",
      "28  Merchant_29  Diamond  35786\n",
      "29  Merchant_30     Gold  11673\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Corrected mock DataFrame\n",
    "data = {\n",
    "    \"Merchant\": [f\"Merchant_{i}\" for i in range(1, 31)],\n",
    "    \"Item\": [\n",
    "        \"Gold\",\"Diamond\",\"Gold\",\"Horse\",\"Spices\",\"Gold\",\"Diamond\",\"Silk\",\"Gold\",\"Diamond\",\n",
    "        \"Diamond\",\"Diamond\",\"Silk\",\"Horse\",\"Spices\",\"Gold\",\"Horse\",\"Diamond\",\"Silk\",\"Horse\",\n",
    "        \"Spices\",\"Gold\",\"Silk\",\"Silk\",\"Gold\",\"Spices\",\"Horse\",\"Gold\",\"Diamond\",\"Gold\"\n",
    "    ],\n",
    "    \"Price\": [\n",
    "        2970,17552,30984,47965,26610,12397,38861,40106,29126,25728,\n",
    "        15406,7470,28742,42008,38352,24878,9986,26779,10751,37558,\n",
    "        21604,36990,35766,2419,35817,49043,20236,27408,35786,11673\n",
    "    ]\n",
    "}\n",
    "\n",
    "bazaar_df = pd.DataFrame(data)\n",
    "\n",
    "# Show first 10 rows\n",
    "first_10 = bazaar_df.head(10)\n",
    "\n",
    "# Show last 5 rows\n",
    "last_5 = bazaar_df.tail(5)\n",
    "\n",
    "print(\"First 10 stalls:\\n\", first_10, \"\\n\")\n",
    "print(\"Last 5 stalls:\\n\", last_5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd0d249",
   "metadata": {},
   "source": [
    "Scene 3: The Palace Guard’s Riddle A guard stops you at the inner sanctum. “Who brings tribute from the House of Saluva?” he demands. Only by presenting the right record can you pass.\n",
    "\n",
    "Task: Use .loc to fetch rows where House == “Saluva”. Handle cases where multiple entries exist. (Concept: Label-based indexing + handling duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b188e623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Tribute Records:\n",
      "       House  Tribute\n",
      "0   Sangama     5000\n",
      "1    Saluva     4200\n",
      "2    Tuluva     4800\n",
      "3  Aravindu     5100\n",
      "4    Vijaya     4700\n",
      "5      Rama     4500\n",
      "6      Hari     4200\n",
      "7      Deva     4600\n",
      "8      Vira     4900\n",
      "9   Chandra     4400 \n",
      "\n",
      "Guard’s Riddle - Tribute from House of Saluva:\n",
      "     House  Tribute\n",
      "1  Saluva     4200\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "families = [\"Sangama\", \"Saluva\", \"Tuluva\", \"Aravindu\",\n",
    "            \"Vijaya\", \"Rama\", \"Hari\", \"Deva\", \"Vira\", \"Chandra\"]\n",
    "\n",
    "tributes = [5000, 4200, 4800, 5100, 4700,\n",
    "            4500, 4200, 4600, 4900, 4400]\n",
    "\n",
    "# Create DataFrame\n",
    "tribute_df = pd.DataFrame({\n",
    "    \"House\": families,\n",
    "    \"Tribute\": tributes\n",
    "})\n",
    "\n",
    "print(\"All Tribute Records:\\n\", tribute_df, \"\\n\")\n",
    "\n",
    "# Guard’s question: Fetch all rows where House == \"Saluva\"\n",
    "saluvan_records = tribute_df.loc[tribute_df[\"House\"] == \"Saluva\"]\n",
    "\n",
    "print(\"Guard’s Riddle - Tribute from House of Saluva:\\n\", saluvan_records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d608bfda",
   "metadata": {},
   "source": [
    "Scene 4: Tungabhadra River Ferry The ferryman demands to know which merchants are second, seventh, and last in the scroll of traders. Only then will he row you across the river.\n",
    "\n",
    " Task: Use .iloc to get the 2nd, 7th, and last row. Then reverse the order of the DataFrame. (Concept: Position-based indexing + slicing tricks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "090cffc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2nd Merchant:\n",
      " Merchant    Merchant_2\n",
      "Item           Diamond\n",
      "Price            17552\n",
      "Name: 1, dtype: object\n",
      "\n",
      "7th Merchant:\n",
      " Merchant    Merchant_7\n",
      "Item           Diamond\n",
      "Price            38861\n",
      "Name: 6, dtype: object\n",
      "\n",
      "Last Merchant:\n",
      " Merchant    Merchant_30\n",
      "Item               Gold\n",
      "Price             11673\n",
      "Name: 29, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Corrected mock DataFrame\n",
    "data = {\n",
    "    \"Merchant\": [f\"Merchant_{i}\" for i in range(1, 31)],\n",
    "    \"Item\": [\n",
    "        \"Gold\",\"Diamond\",\"Gold\",\"Horse\",\"Spices\",\"Gold\",\"Diamond\",\"Silk\",\"Gold\",\"Diamond\",\n",
    "        \"Diamond\",\"Diamond\",\"Silk\",\"Horse\",\"Spices\",\"Gold\",\"Horse\",\"Diamond\",\"Silk\",\"Horse\",\n",
    "        \"Spices\",\"Gold\",\"Silk\",\"Silk\",\"Gold\",\"Spices\",\"Horse\",\"Gold\",\"Diamond\",\"Gold\"\n",
    "    ],\n",
    "    \"Price\": [\n",
    "        2970,17552,30984,47965,26610,12397,38861,40106,29126,25728,\n",
    "        15406,7470,28742,42008,38352,24878,9986,26779,10751,37558,\n",
    "        21604,36990,35766,2419,35817,49043,20236,27408,35786,11673\n",
    "    ]\n",
    "}\n",
    "\n",
    "bazaar_df = pd.DataFrame(data)\n",
    "second = bazaar_df.iloc[1]     # 2nd row (index 1)\n",
    "seventh = bazaar_df.iloc[6]    # 7th row (index 6)\n",
    "last = bazaar_df.iloc[-1]      # last row\n",
    "\n",
    "print(\"\\n2nd Merchant:\\n\", second)\n",
    "print(\"\\n7th Merchant:\\n\", seventh)\n",
    "print(\"\\nLast Merchant:\\n\", last)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4825b7f8",
   "metadata": {},
   "source": [
    "Scene 5: Spice Ships at Goa Port A Portuguese captain offers you a CSV file of spices exported from Vijayanagara: quantity, type, and price. Some quantities are decimals, but you need integers.\n",
    "\n",
    "Task: Import the CSV, display top 10 rows, and convert “Quantity” to integer type. (Concept: Import CSV + type conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "fe7f5fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 rows:\n",
      "        Spice  Quantity  Price\n",
      "0     Pepper      12.5    200\n",
      "1   Cardamom       7.3    500\n",
      "2      Clove       3.9    300\n",
      "3   Cinnamon       5.0    400\n",
      "4     Nutmeg       8.5    250\n",
      "5   Turmeric      10.2    150\n",
      "6    Saffron       1.5   1000\n",
      "7  Coriander       9.0    120\n",
      "\n",
      "After converting 'Quantity' to integer:\n",
      "        Spice  Quantity  Price\n",
      "0     Pepper        12    200\n",
      "1   Cardamom         7    500\n",
      "2      Clove         3    300\n",
      "3   Cinnamon         5    400\n",
      "4     Nutmeg         8    250\n",
      "5   Turmeric        10    150\n",
      "6    Saffron         1   1000\n",
      "7  Coriander         9    120\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Mount Google Drive\n",
    "\n",
    "# Path to your CSV in Google Drivecsv\n",
    "file_path = './vijayanagara_datasets/spices.csv'\n",
    "\n",
    "# Read CSV\n",
    "spices_df = pd.read_csv(file_path)\n",
    "\n",
    "# Display top 10 rows\n",
    "print(\"Top 10 rows:\\n\", spices_df.head(10))\n",
    "\n",
    "# Convert Quantity to integer\n",
    "spices_df[\"Quantity\"] = spices_df[\"Quantity\"].astype(int)\n",
    "print(\"\\nAfter converting 'Quantity' to integer:\\n\", spices_df.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0866d7",
   "metadata": {},
   "source": [
    "Scene 6: Temple Treasury Records In Tirupati, temple priests reveal Excel files recording gold, land, and donations in multiple sheets. Each sheet tells only part of the story.\n",
    "\n",
    "Task: Load all sheets into separate DataFrames, then concatenate them into one DataFrame.\n",
    "\n",
    "Add a column indicating “Source” (Gold, Land, Donations).\n",
    "\n",
    "(Concept: Import Excel + concat + adding a column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dd4901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Update the file path to include the full file name (including extension)\n",
    "excel_file = \"./vijayanagara_datasets/treasury.xlsx\"\n",
    "\n",
    "# Read all sheets into a dictionary of DataFrames\n",
    "dfs = pd.read_excel(excel_file, sheet_name=None)\n",
    "\n",
    "# List of source labels for each sheet (assuming the order is Gold, Land, Donations)\n",
    "sources = ['Gold', 'Land', 'Donations']\n",
    "\n",
    "# Empty list to hold all the DataFrames with the 'Source' column\n",
    "df_list = []\n",
    "\n",
    "# Iterate over the dictionary of DataFrames\n",
    "for idx, (sheet_name, df) in enumerate(dfs.items()):\n",
    "    # Add a new column 'Source' based on the sheet order\n",
    "    df['Source'] = sources[idx]\n",
    "\n",
    "    # Append the DataFrame to the list\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "final_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(final_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1435828",
   "metadata": {},
   "source": [
    "Scene 7: Royal Granaries of Andhra Rice production scrolls show Area and Productivity, but some values seem exaggerated, hinting at errors.\n",
    "\n",
    " Task: Add a Total_Yield column = Area × Productivity. Identify outliers (> mean + 2*std) and filter them out.\n",
    "\n",
    "  (Concept: Column operations + statistical filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "11ba266a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With outliers:\n",
      "         Region  Area  Productivity  Total_Yield\n",
      "0       Andhra   146            47         6862\n",
      "1    Karnataka   131            66         8646\n",
      "2    TamilNadu    90            68         6120\n",
      "3    Telangana    94            54         5076\n",
      "4       Kerala   107            44         4708\n",
      "5          Goa   134            61         8174\n",
      "6  Maharashtra   115            60         6900\n",
      "7       Odisha   123            48         5904\n",
      "8        Bihar    83            48         3984\n",
      "9        Assam   120            65         7800 \n",
      "\n",
      "After removing outliers:\n",
      "       Region  Area  Productivity  Total_Yield\n",
      "0     Andhra   146            47         6862\n",
      "1  Karnataka   131            66         8646\n",
      "2  TamilNadu    90            68         6120\n",
      "3  Telangana    94            54         5076\n",
      "4     Kerala   107            44         4708\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file instead of Excel\n",
    "df = pd.read_csv(\"./vijayanagara_datasets/granaries.csv\")\n",
    "\n",
    "# 1. Add Total_Yield = Area × Productivity\n",
    "df[\"Total_Yield\"] = df[\"Area\"] * df[\"Productivity\"]\n",
    "\n",
    "# 2. Calculate mean and std of Total_Yield\n",
    "mean_yield = df[\"Total_Yield\"].mean()\n",
    "std_yield = df[\"Total_Yield\"].std()\n",
    "\n",
    "# 3. Define threshold for outliers (mean + 2*std)\n",
    "threshold = mean_yield + 2 * std_yield\n",
    "\n",
    "# 4. Filter out rows where Total_Yield > threshold\n",
    "filtered_df = df[df[\"Total_Yield\"] <= threshold]\n",
    "\n",
    "# Show results\n",
    "print(\"With outliers:\\n\", df, \"\\n\")\n",
    "print(\"After removing outliers:\\n\", filtered_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8520d693",
   "metadata": {},
   "source": [
    "Scene 8: Horse Traders of Hampi Arab horses, Persian horses, and fake horses crowd the marketplace. Only genuine Arab horses cost more than 2000 gold coins.\n",
    "\n",
    " Task: Filter rows where Price > 2000 & Origin != \"Fake\". Sort descending by price. (Concept: Filtering with conditions + sorting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "10de3fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All horses in the bazaar:\n",
      "       Trader HorseName   Origin  Price\n",
      "0   Trader_1     Arjun     Arab   3000\n",
      "1   Trader_2    Bheema  Persian   2500\n",
      "2   Trader_3   Chandra     Fake    500\n",
      "3   Trader_4     Drona     Arab   4000\n",
      "4   Trader_5   Ekalvya     Arab   3500\n",
      "5   Trader_6     Feroz     Arab   2800\n",
      "6   Trader_7     Gopal  Persian    700\n",
      "7   Trader_8      Hari     Fake   3200\n",
      "8   Trader_9     Indra     Arab   2600\n",
      "9  Trader_10       Jai  Persian   3400\n",
      "\n",
      "Genuine horses (sorted by price):\n",
      "       Trader HorseName   Origin  Price\n",
      "3   Trader_4     Drona     Arab   4000\n",
      "4   Trader_5   Ekalvya     Arab   3500\n",
      "9  Trader_10       Jai  Persian   3400\n",
      "0   Trader_1     Arjun     Arab   3000\n",
      "5   Trader_6     Feroz     Arab   2800\n",
      "8   Trader_9     Indra     Arab   2600\n",
      "1   Trader_2    Bheema  Persian   2500\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"Trader\": [f\"Trader_{i}\" for i in range(1, 11)],\n",
    "    \"HorseName\": [\"Arjun\", \"Bheema\", \"Chandra\", \"Drona\", \"Ekalvya\",\n",
    "                  \"Feroz\", \"Gopal\", \"Hari\", \"Indra\", \"Jai\"],\n",
    "    \"Origin\": [\"Arab\", \"Persian\", \"Fake\", \"Arab\", \"Arab\",\n",
    "               \"Arab\", \"Persian\", \"Fake\", \"Arab\", \"Persian\"],\n",
    "    \"Price\": [3000, 2500, 500, 4000, 3500, 2800, 700, 3200, 2600, 3400]\n",
    "}\n",
    "\n",
    "horses_df = pd.DataFrame(data)\n",
    "print(\"All horses in the bazaar:\\n\", horses_df)\n",
    "# Filter genuine horses (Price > 2000, not Fake)\n",
    "genuine_horses = horses_df.loc[\n",
    "    (horses_df[\"Price\"] > 2000) & (horses_df[\"Origin\"] != \"Fake\")\n",
    "]\n",
    "\n",
    "# Sort descending by price\n",
    "genuine_horses_sorted = genuine_horses.sort_values(by=\"Price\", ascending=False)\n",
    "\n",
    "print(\"\\nGenuine horses (sorted by price):\\n\", genuine_horses_sorted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d75017d",
   "metadata": {},
   "source": [
    "Scene 9: Monsoon Scrolls of Vijayanagara Temple inscriptions record rainfall for irrigation tanks in different regions. The priests need the wettest regions for planning.\n",
    "\n",
    "Task: Group by Region and compute mean, max, and min rainfall. Return the top 3 wettest regions. (Concept: GroupBy + multiple aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "44361b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temple Rainfall Records:\n",
      "         Region  Rainfall\n",
      "0  Vijaynagara      1079\n",
      "1        Hampi      1400\n",
      "2     Anegundi      1506\n",
      "3       Kampli      1349\n",
      "4  Tungabhadra      1102\n",
      "5       Badami      1176\n",
      "6    Gulbaraga      1571\n",
      "7      Bellary      1179\n",
      "8       Hospet      1241\n",
      "9     Anatapur      1076\n",
      "\n",
      "Rainfall summary by region:\n",
      "         Region    mean   max   min\n",
      "0     Anatapur  1076.0  1076  1076\n",
      "1     Anegundi  1506.0  1506  1506\n",
      "2       Badami  1176.0  1176  1176\n",
      "3      Bellary  1179.0  1179  1179\n",
      "4    Gulbaraga  1571.0  1571  1571\n",
      "5        Hampi  1400.0  1400  1400\n",
      "6       Hospet  1241.0  1241  1241\n",
      "7       Kampli  1349.0  1349  1349\n",
      "8  Tungabhadra  1102.0  1102  1102\n",
      "9  Vijaynagara  1079.0  1079  1079\n",
      "\n",
      "Top 3 wettest regions:\n",
      "       Region    mean   max   min\n",
      "4  Gulbaraga  1571.0  1571  1571\n",
      "1   Anegundi  1506.0  1506  1506\n",
      "5      Hampi  1400.0  1400  1400\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"Region\": [\n",
    "        \"Vijaynagara\",\"Hampi\",\"Anegundi\",\"Kampli\",\"Tungabhadra\",\n",
    "        \"Badami\",\"Gulbaraga\",\"Bellary\",\"Hospet\",\"Anatapur\"\n",
    "    ],\n",
    "    \"Rainfall\": [1079,1400,1506,1349,1102,1176,1571,1179,1241,1076]\n",
    "}\n",
    "\n",
    "rain_df = pd.DataFrame(data)\n",
    "print(\"Temple Rainfall Records:\\n\", rain_df)\n",
    "\n",
    "# Group by Region and aggregate\n",
    "rain_stats = rain_df.groupby(\"Region\")[\"Rainfall\"].agg([\"mean\", \"max\", \"min\"]).reset_index()\n",
    "\n",
    "# Sort by mean rainfall, pick top 3 regions\n",
    "top_3_regions = rain_stats.sort_values(by=\"mean\", ascending=False).head(3)\n",
    "\n",
    "print(\"\\nRainfall summary by region:\\n\", rain_stats)\n",
    "print(\"\\nTop 3 wettest regions:\\n\", top_3_regions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ed6181",
   "metadata": {},
   "source": [
    "Scene 10: Diamond Merchants of Golconda Two ledgers record diamond trade separately. You must know total trade per region to unlock the merchant guild’s secret.\n",
    "\n",
    " Task: Concatenate the two DataFrames vertically. Add a column Ledger_Source. (Concept: Concatenate + source tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c39c732a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ledger 1:\n",
      "          Region  Diamonds\n",
      "0  Vijayanagara        50\n",
      "1      Golconda        70\n",
      "2         Hampi        60\n",
      "3      Anegundi        40\n",
      "4        Kampli        55 \n",
      "\n",
      "Ledger 2:\n",
      "          Region  Diamonds\n",
      "0  Vijayanagara        45\n",
      "1      Golconda        65\n",
      "2         Hampi        62\n",
      "3        Badami        38\n",
      "4       Bellary        50\n",
      "\n",
      "Combined Ledgers:\n",
      "          Region  Diamonds Ledger_Source\n",
      "0  Vijayanagara        50      Ledger 1\n",
      "1      Golconda        70      Ledger 1\n",
      "2         Hampi        60      Ledger 1\n",
      "3      Anegundi        40      Ledger 1\n",
      "4        Kampli        55      Ledger 1\n",
      "5  Vijayanagara        45      Ledger 2\n",
      "6      Golconda        65      Ledger 2\n",
      "7         Hampi        62      Ledger 2\n",
      "8        Badami        38      Ledger 2\n",
      "9       Bellary        50      Ledger 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ledger 1\n",
    "data1 = {\n",
    "    \"Region\": [\"Vijayanagara\", \"Golconda\", \"Hampi\", \"Anegundi\",\"Kampli\"],\n",
    "    \"Diamonds\": [50,70,60,40,55]\n",
    "}\n",
    "ledger1_df = pd.DataFrame(data1)\n",
    "\n",
    "# Ledger 2\n",
    "data2 = {\n",
    "    \"Region\": [\"Vijayanagara\",\"Golconda\", \"Hampi\", \"Badami\", \"Bellary\"],\n",
    "    \"Diamonds\": [45,65,62,38,50]\n",
    "}\n",
    "ledger2_df = pd.DataFrame(data2)\n",
    "\n",
    "print(\"Ledger 1:\\n\", ledger1_df, \"\\n\")\n",
    "print(\"Ledger 2:\\n\", ledger2_df)\n",
    "# Add source column\n",
    "ledger1_df[\"Ledger_Source\"] = \"Ledger 1\"\n",
    "ledger2_df[\"Ledger_Source\"] = \"Ledger 2\"\n",
    "\n",
    "# Concatenate vertically\n",
    "combined = pd.concat([ledger1_df, ledger2_df], ignore_index=True)\n",
    "\n",
    "print(\"\\nCombined Ledgers:\\n\", combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80c9da5",
   "metadata": {},
   "source": [
    "Scene 11: Scholar of Hampi A scholar presents two scrolls: one with student names, another with marks. Some students are missing in one scroll.\n",
    "\n",
    "Task: Merge the DataFrames on Roll_Number.\n",
    "\n",
    "Compare inner, left, and right joins. (Concept: Merge types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "86290443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scroll 1 (Names):\n",
      "    Roll_Number   Name\n",
      "0          101   Ravi\n",
      "1          102  Anand\n",
      "2          103  Meera\n",
      "3          104   Sita\n",
      "4          105   Hari \n",
      "\n",
      "Scroll 2 (Marks):\n",
      "    Roll_Number  Marks\n",
      "0          101     85\n",
      "1          102     90\n",
      "2          104     88\n",
      "3          106     92\n",
      "4          107     80 \n",
      "\n",
      "INNER JOIN:\n",
      "    Roll_Number   Name  Marks\n",
      "0          101   Ravi     85\n",
      "1          102  Anand     90\n",
      "2          104   Sita     88 \n",
      "\n",
      "LEFT JOIN:\n",
      "    Roll_Number   Name  Marks\n",
      "0          101   Ravi   85.0\n",
      "1          102  Anand   90.0\n",
      "2          103  Meera    NaN\n",
      "3          104   Sita   88.0\n",
      "4          105   Hari    NaN \n",
      "\n",
      "RIGHT JOIN:\n",
      "    Roll_Number   Name  Marks\n",
      "0          101   Ravi     85\n",
      "1          102  Anand     90\n",
      "2          104   Sita     88\n",
      "3          106    NaN     92\n",
      "4          107    NaN     80 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Scroll 1: Student details\n",
    "df_names = pd.DataFrame({\n",
    "    \"Roll_Number\": [101, 102, 103, 104,105],\n",
    "    \"Name\": [\"Ravi\", \"Anand\", \"Meera\", \"Sita\",\"Hari\"]\n",
    "})\n",
    "\n",
    "# Scroll 2: Marks details\n",
    "df_marks = pd.DataFrame({\n",
    "    \"Roll_Number\": [101, 102, 104,106,107],\n",
    "    \"Marks\": [85,90,88,92,80]\n",
    "})\n",
    "\n",
    "print(\"Scroll 1 (Names):\\n\", df_names, \"\\n\")\n",
    "print(\"Scroll 2 (Marks):\\n\", df_marks, \"\\n\")\n",
    "\n",
    "# 1. INNER JOIN → only matching Roll_Number\n",
    "inner_merge = pd.merge(df_names, df_marks, on=\"Roll_Number\", how=\"inner\")\n",
    "\n",
    "# 2. LEFT JOIN → keep all from df_names, add marks if available\n",
    "left_merge = pd.merge(df_names, df_marks, on=\"Roll_Number\", how=\"left\")\n",
    "\n",
    "# 3. RIGHT JOIN → keep all from df_marks, add names if available\n",
    "right_merge = pd.merge(df_names, df_marks, on=\"Roll_Number\", how=\"right\")\n",
    "\n",
    "print(\"INNER JOIN:\\n\", inner_merge, \"\\n\")\n",
    "print(\"LEFT JOIN:\\n\", left_merge, \"\\n\")\n",
    "print(\"RIGHT JOIN:\\n\", right_merge, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2105f37",
   "metadata": {},
   "source": [
    "Scene 12: Caravanserai of Bijapur Merchants’ wool and cotton trade data share the same index, but some regions are missing records.\n",
    "\n",
    " Task: Join the DataFrames on index and fill missing values with 0.\n",
    "\n",
    "  (Concept: Join + handling NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "72cafb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wool Data:\n",
      "              Wool\n",
      "Vijaynagara   500\n",
      "Hampi         450\n",
      "Anegundi      300\n",
      "Kampli        400\n",
      "Badami        350 \n",
      "\n",
      "Cotton Data:\n",
      "              Cotton\n",
      "Vijaynagara     600\n",
      "Hampi           400\n",
      "Anegundi        320\n",
      "Bellary         380\n",
      "Hospet          300 \n",
      "\n",
      "Final Trade Data (with missing = 0):\n",
      "               Wool  Cotton\n",
      "Anegundi     300.0   320.0\n",
      "Badami       350.0     0.0\n",
      "Bellary        0.0   380.0\n",
      "Hampi        450.0   400.0\n",
      "Hospet         0.0   300.0\n",
      "Kampli       400.0     0.0\n",
      "Vijaynagara  500.0   600.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Caravanserai scroll 1: Wool trade\n",
    "wool_df = pd.DataFrame({\n",
    "    \"Wool\": [500, 450, 300,400,350]\n",
    "}, index=[\"Vijaynagara\", \"Hampi\",\"Anegundi\",\"Kampli\",\"Badami\"])\n",
    "\n",
    "# Caravanserai scroll 2: Cotton trade\n",
    "cotton_df = pd.DataFrame({\n",
    "    \"Cotton\": [600, 400, 320,380,300]\n",
    "}, index=[\"Vijaynagara\", \"Hampi\",\"Anegundi\",\"Bellary\",\"Hospet\"])\n",
    "\n",
    "print(\"Wool Data:\\n\", wool_df, \"\\n\")\n",
    "print(\"Cotton Data:\\n\", cotton_df, \"\\n\")\n",
    "\n",
    "# 1. Join on index (outer join by default keeps all regions)\n",
    "trade_df = wool_df.join(cotton_df, how=\"outer\")\n",
    "\n",
    "# 2. Fill missing values with 0\n",
    "trade_df = trade_df.fillna(0)\n",
    "\n",
    "print(\"Final Trade Data (with missing = 0):\\n\", trade_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6c6e65",
   "metadata": {},
   "source": [
    "Scene 13: Navaratri Festival Stalls Festival stalls sell rice, sweets, silk, and horses. You are asked to analyze only sweets and silk.\n",
    "\n",
    "Task: Use .isin() to filter categories.\n",
    "\n",
    "Compute total revenue for these categories. (Concept: Filtering with isin + aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e860d9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Festival Stalls (Sweets & Silk):\n",
      "    Category  Revenue\n",
      "0    Sweets     4526\n",
      "4    Sweets     1966\n",
      "5      Silk     4998\n",
      "6      Silk      790\n",
      "10     Silk     2710\n",
      "11     Silk     3434\n",
      "15     Silk     1751\n",
      "17     Silk     2901\n",
      "19     Silk     4590\n",
      "\n",
      "Total Revenue from Sweets and Silk: 27666\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Festival stalls dataset\n",
    "data = {\n",
    "    \"Category\": [\n",
    "        \"Sweets\",\"Horse\",\"Horse\",\"Horse\",\"Sweets\",\"Silk\",\"Silk\",\"Rice\",\"Rice\",\"Rice\",\n",
    "        \"Silk\",\"Silk\",\"Horse\",\"Horse\",\"Rice\",\"Silk\",\"Rice\",\"Silk\",\"Rice\",\"Silk\"\n",
    "    ],\n",
    "    \"Revenue\": [\n",
    "        4526,3121,2387,1647,1966,4998,790,3284,1279,669,\n",
    "        2710,3434,4490,2134,3093,1751,2241,2901,1852,4590\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 1. Filter only Sweets and Silk\n",
    "filtered_df = df[df[\"Category\"].isin([\"Sweets\", \"Silk\"])]\n",
    "\n",
    "# 2. Compute total revenue for these categories\n",
    "total_revenue = filtered_df[\"Revenue\"].sum()\n",
    "\n",
    "print(\"Filtered Festival Stalls (Sweets & Silk):\\n\", filtered_df)\n",
    "print(\"\\nTotal Revenue from Sweets and Silk:\", total_revenue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375cfa3f",
   "metadata": {},
   "source": [
    "Scene 14: Census of Vijayanagara Officials record populations of provinces. Some provinces are missing entries.\n",
    "\n",
    " Task: Find the top 5 populated provinces.\n",
    "\n",
    "  Compute their percentage contribution to total population. (Concept: Sorting + percentage calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "bcba5d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Population of  Provinces:\n",
      "        Province  Population\n",
      "3        Kampli     1493184\n",
      "6        Hospet     1391628\n",
      "1         Hampi     1356381\n",
      "7   Tungabhadra     1334427\n",
      "9      Gulbarga     1306462\n",
      "0  Vijayanagara     1058578\n",
      "5       Bellary      818907\n",
      "8     Anantapur      675255\n",
      "2      Anegundi      644054\n",
      "4        Badami      591633\n",
      "Top 5 Populated Provinces:\n",
      "       Province  Population  Percentage\n",
      "3       Kampli     1493184   13.993559\n",
      "6       Hospet     1391628   13.041815\n",
      "1        Hampi     1356381   12.711493\n",
      "7  Tungabhadra     1334427   12.505748\n",
      "9     Gulbarga     1306462   12.243671\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Vijayanagara census dataset\n",
    "data = {\n",
    "    \"Province\": [\n",
    "        \"Vijayanagara\",\"Hampi\",\"Anegundi\",\"Kampli\",\"Badami\",\n",
    "        \"Bellary\",\"Hospet\",\"Tungabhadra\",\"Anantapur\",\"Gulbarga\"\n",
    "    ],\n",
    "    \"Population\": [\n",
    "        1058578,1356381,644054,1493184,591633,\n",
    "        818907,1391628,1334427,675255,1306462\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 1. Sort provinces by population descending\n",
    "df_sorted = df.sort_values(by=\"Population\", ascending=False)\n",
    "print(\" Population of  Provinces:\\n\", df_sorted)\n",
    "# 2. Take top 5 populated provinces\n",
    "# Take top 5 populated provinces and make a copy\n",
    "top5 = df_sorted.head(5).copy()\n",
    "\n",
    "# Compute percentage contribution\n",
    "total_population = df[\"Population\"].sum()\n",
    "top5[\"Percentage\"] = (top5[\"Population\"] / total_population) * 100\n",
    "\n",
    "# Display results\n",
    "print(\"Top 5 Populated Provinces:\\n\", top5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d2acc0",
   "metadata": {},
   "source": [
    "Scene 14: Census of Vijayanagara Officials record populations of provinces. Some provinces are missing entries.\n",
    "\n",
    " Task: Find the top 5 populated provinces.\n",
    "\n",
    "  Compute their percentage contribution to total population. (Concept: Sorting + percentage calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ad29a66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Population of  Provinces:\n",
      "        Province  Population\n",
      "3        Kampli     1493184\n",
      "6        Hospet     1391628\n",
      "1         Hampi     1356381\n",
      "7   Tungabhadra     1334427\n",
      "9      Gulbarga     1306462\n",
      "0  Vijayanagara     1058578\n",
      "5       Bellary      818907\n",
      "8     Anantapur      675255\n",
      "2      Anegundi      644054\n",
      "4        Badami      591633\n",
      "Top 5 Populated Provinces:\n",
      "       Province  Population  Percentage\n",
      "3       Kampli     1493184   13.993559\n",
      "6       Hospet     1391628   13.041815\n",
      "1        Hampi     1356381   12.711493\n",
      "7  Tungabhadra     1334427   12.505748\n",
      "9     Gulbarga     1306462   12.243671\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Vijayanagara census dataset\n",
    "data = {\n",
    "    \"Province\": [\n",
    "        \"Vijayanagara\",\"Hampi\",\"Anegundi\",\"Kampli\",\"Badami\",\n",
    "        \"Bellary\",\"Hospet\",\"Tungabhadra\",\"Anantapur\",\"Gulbarga\"\n",
    "    ],\n",
    "    \"Population\": [\n",
    "        1058578,1356381,644054,1493184,591633,\n",
    "        818907,1391628,1334427,675255,1306462\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 1. Sort provinces by population descending\n",
    "df_sorted = df.sort_values(by=\"Population\", ascending=False)\n",
    "print(\" Population of  Provinces:\\n\", df_sorted)\n",
    "# 2. Take top 5 populated provinces\n",
    "# Take top 5 populated provinces and make a copy\n",
    "top5 = df_sorted.head(5).copy()\n",
    "\n",
    "# Compute percentage contribution\n",
    "total_population = df[\"Population\"].sum()\n",
    "top5[\"Percentage\"] = (top5[\"Population\"] / total_population) * 100\n",
    "\n",
    "# Display results\n",
    "print(\"Top 5 Populated Provinces:\\n\", top5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52926c18",
   "metadata": {},
   "source": [
    "Scene 16: Court Poets of HampiRevenue from drama performances is recorded by year.\n",
    "\n",
    "Task: Group by Year, compute total revenue, and\n",
    "\n",
    " find the highest-grossing drama per year using groupby().apply(). (Concept: GroupBy + apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4d2a1a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Revenue per Year:\n",
      "    Year  Total_Revenue\n",
      "0  1520          11000\n",
      "1  1521          13500\n",
      "2  1522          10000\n",
      "\n",
      "Highest-Grossing Drama per Year:\n",
      "    Year         Drama  Revenue\n",
      "0  1520   Mahabharata     6000\n",
      "1  1521  KrishnaLeela     7000\n",
      "2  1522      Kalidasa     5200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_13520\\3388886814.py:19: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top_drama_per_year = df.groupby(\"Year\").apply(highest_grossing).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Court Poets dataset\n",
    "data = {\n",
    "    \"Year\": [1520, 1520, 1521, 1521, 1522, 1522],\n",
    "    \"Drama\": [\"Ramayana\", \"Mahabharata\", \"KrishnaLeela\", \"Bhagavata\", \"Raghuvamsha\", \"Kalidasa\"],\n",
    "    \"Revenue\": [5000, 6000, 7000, 6500, 4800, 5200]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 1. Group by Year and compute total revenue\n",
    "total_revenue_per_year = df.groupby(\"Year\")[\"Revenue\"].sum().reset_index(name=\"Total_Revenue\")\n",
    "\n",
    "# 2. Find highest-grossing drama per year\n",
    "def highest_grossing(group):\n",
    "    return group.loc[group[\"Revenue\"].idxmax()]\n",
    "\n",
    "top_drama_per_year = df.groupby(\"Year\").apply(highest_grossing).reset_index(drop=True)\n",
    "\n",
    "# Display results\n",
    "print(\"Total Revenue per Year:\\n\", total_revenue_per_year)\n",
    "print(\"\\nHighest-Grossing Drama per Year:\\n\", top_drama_per_year)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305d82c3",
   "metadata": {},
   "source": [
    "Scene 17: Royal Kitchen of Vijayanagara Feast recipe data contains missing Spices. You must ensure no feast is incomplete.\n",
    "\n",
    "Task: Fill missing Spice with \"Unknown\".\n",
    "\n",
    " Forward-fill Quantity column. (Concept: fillna with method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0bc2f96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Spice  Quantity  Price\n",
      "0     Pepper      12.5    200\n",
      "1   Cardamom       7.3    500\n",
      "2      Clove       3.9    300\n",
      "3   Cinnamon       5.0    400\n",
      "4     Nutmeg       8.5    250\n",
      "5   Turmeric      10.2    150\n",
      "6    Saffron       1.5   1000\n",
      "7  Coriander       9.0    120\n",
      "8    Unknown       9.0    500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_13520\\1141090316.py:16: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[\"Quantity\"] = df[\"Quantity\"].fillna(method=\"ffill\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Feast recipe dataset\n",
    "data = {\n",
    "    \"Spice\": [\"Pepper\",\"Cardamom\",\"Clove\",\"Cinnamon\",\"Nutmeg\",\"Turmeric\",\"Saffron\",\"Coriander\", None],\n",
    "    \"Quantity\": [12.5, 7.3, 3.9, 5.0, 8.5, 10.2, 1.5, 9.0, None],\n",
    "    \"Price\": [200, 500, 300, 400, 250, 150, 1000, 120, 500]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 1. Fill missing Spice with \"Unknown\"\n",
    "df[\"Spice\"] = df[\"Spice\"].fillna(\"Unknown\")\n",
    "\n",
    "# 2. Forward-fill Quantity\n",
    "df[\"Quantity\"] = df[\"Quantity\"].fillna(method=\"ffill\")\n",
    "\n",
    "# Display the cleaned dataset\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b102760b",
   "metadata": {},
   "source": [
    "Scene 18: Temple Inscriptions Donation inscriptions are sometimes repeated by mistake.\n",
    "\n",
    " Task: Drop duplicates, reset index, and rename columns appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "bf9f2217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Donor_Name  Donation_Amount\n",
      "0       Ravi              500\n",
      "1      Anand              700\n",
      "2      Meera              600\n",
      "3       Sita              550\n",
      "4       Hari              620\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Temple donation inscriptions dataset\n",
    "data = {\n",
    "    \"Donor\": [\"Ravi\", \"Anand\", \"Ravi\", \"Meera\", \"Sita\", \"Hari\"],\n",
    "    \"Amount\": [500, 700, 500, 600, 550, 620]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 1. Drop duplicate rows\n",
    "df_clean = df.drop_duplicates()\n",
    "\n",
    "# 2. Reset the index\n",
    "df_clean = df_clean.reset_index(drop=True)\n",
    "\n",
    "# 3. Rename columns (optional, make names title case)\n",
    "df_clean = df_clean.rename(columns={\"Donor\": \"Donor_Name\", \"Amount\": \"Donation_Amount\"})\n",
    "\n",
    "# Display the cleaned dataset\n",
    "print(df_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159f4029",
   "metadata": {},
   "source": [
    "Scene 19: Assembly of Data Scrolls After collecting 19 scrolls, you must merge them into one master dataset.\n",
    "\n",
    "Task: Merge/concatenate all scrolls. Drop duplicates. Summarize Treasure_Value by Region.\n",
    "\n",
    " (Concept: Large-scale merging + cleanup + final aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "0e41a1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treasure Value Summary by Region:\n",
      "           Region  Treasure_Value\n",
      "0      Anantapur             0.0\n",
      "1         Andhra             0.0\n",
      "2       Anegundi           620.0\n",
      "3          Assam             0.0\n",
      "4         Badami           350.0\n",
      "5        Bellary           380.0\n",
      "6          Bihar             0.0\n",
      "7            Goa             0.0\n",
      "8       Golconda             0.0\n",
      "9       Gulbarga             0.0\n",
      "10         Hampi           850.0\n",
      "11        Hospet           300.0\n",
      "12        Kampli           400.0\n",
      "13     Karnataka             0.0\n",
      "14        Kerala             0.0\n",
      "15   Maharashtra             0.0\n",
      "16        Odisha             0.0\n",
      "17     TamilNadu             0.0\n",
      "18     Telangana             0.0\n",
      "19   Tungabhadra             0.0\n",
      "20  Vijayanagara          1100.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Simulate 19 scrolls\n",
    "# -----------------------------\n",
    "\n",
    "# 1st scroll: Stall, Item, Price\n",
    "scroll1 = pd.DataFrame({\n",
    "    \"Stall\": range(1, 31),\n",
    "    \"Item\": [\"Gold\",\"Diamond\",\"Gold\",\"Horse\",\"Spices\",\"Gold\",\"Diamond\",\"Silk\",\"Gold\",\"Diamond\",\n",
    "             \"Diamond\",\"Diamond\",\"Silk\",\"Horse\",\"Spices\",\"Gold\",\"Horse\",\"Diamond\",\"Silk\",\"Horse\",\n",
    "             \"Spices\",\"Gold\",\"Silk\",\"Silk\",\"Gold\",\"Spices\",\"Horse\",\"Gold\",\"Diamond\",\"Gold\"],\n",
    "    \"Price\": [29708,17552,30984,47965,26610,12397,38861,40106,29126,25728,\n",
    "              15406,7470,28742,42008,38352,24878,9986,26779,10751,37558,\n",
    "              21604,36990,35766,2419,35817,49043,20236,27408,35786,11673]\n",
    "})\n",
    "\n",
    "# 2nd scroll: Province, Population\n",
    "scroll2 = pd.DataFrame({\n",
    "    \"Province\": [\"Vijayanagara\",\"Hampi\",\"Anegundi\",\"Kampli\",\"Badami\",\"Bellary\",\"Hospet\",\n",
    "                 \"Tungabhadra\",\"Anantapur\",\"Gulbarga\"],\n",
    "    \"Population\": [1058578,1356381,644054,1493184,591633,818907,1391628,1334427,675255,1306462]\n",
    "})\n",
    "\n",
    "# 3rd scroll: Chariot schedule\n",
    "scroll3 = pd.DataFrame({\n",
    "    \"Chariot\": [\"Sun\",\"Moon\",\"Star\",\"Mars\",\"Venus\"],\n",
    "    \"Departure\": [\"08:00\",\"09:00\",\"10:30\",\"12:00\",\"13:30\"],\n",
    "    \"Arrival\": [\"09:30\",\"10:15\",\"12:00\",\"13:30\",\"15:00\"]\n",
    "})\n",
    "\n",
    "# 4th scroll: Region, Value (treasure)\n",
    "scroll4 = pd.DataFrame({\n",
    "    \"Region\": [\"Vijayanagara\",\"Hampi\",\"Anegundi\",\"Bellary\",\"Hospet\"],\n",
    "    \"Treasure_Value\": [600,400,320,380,300]\n",
    "})\n",
    "\n",
    "# 5th scroll: Region, Diamonds\n",
    "scroll5 = pd.DataFrame({\n",
    "    \"Region\": [\"Vijayanagara\",\"Golconda\",\"Hampi\",\"Anegundi\",\"Kampli\"],\n",
    "    \"Diamonds\": [50,70,60,40,55]\n",
    "})\n",
    "\n",
    "# 6th scroll: Region, Diamonds (another)\n",
    "scroll6 = pd.DataFrame({\n",
    "    \"Region\": [\"Vijayanagara\",\"Golconda\",\"Hampi\",\"Badami\",\"Bellary\"],\n",
    "    \"Diamonds\": [45,65,62,38,50]\n",
    "})\n",
    "\n",
    "# 7th scroll: Donor, Amount\n",
    "scroll7 = pd.DataFrame({\n",
    "    \"Donor\": [\"Ravi\",\"Anand\",\"Ravi\",\"Meera\",\"Sita\",\"Hari\"],\n",
    "    \"Amount\": [500,700,500,600,550,620]\n",
    "})\n",
    "\n",
    "# 8th scroll: Year, Drama, Revenue\n",
    "scroll8 = pd.DataFrame({\n",
    "    \"Year\": [1520,1520,1521,1521,1522,1522],\n",
    "    \"Drama\": [\"Ramayana\",\"Mahabharata\",\"KrishnaLeela\",\"Bhagavata\",\"Raghuvamsha\",\"Kalidasa\"],\n",
    "    \"Revenue\": [5000,6000,7000,6500,4800,5200]\n",
    "})\n",
    "\n",
    "# 9th scroll: House, Tribute\n",
    "scroll9 = pd.DataFrame({\n",
    "    \"House\": [\"Sangama\",\"Saluva\",\"Tuluva\",\"Aravidu\",\"Vijaya\",\"Rama\",\"Hari\",\"Deva\",\"Vira\",\"Chandra\"],\n",
    "    \"Tribute\": [5000,4200,4800,5100,4700,4500,4300,4600,4900,4400]\n",
    "})\n",
    "\n",
    "# 10th scroll: Dish, Spice, Quantity\n",
    "scroll10 = pd.DataFrame({\n",
    "    \"Dish\": [\"Biryani\",\"Biryani\",\"Haleem\",\"Pulao\",\"Haleem\"],\n",
    "    \"Spice\": [\"Saffron\",None,\"Cardamom\",\"Clove\",None],\n",
    "    \"Quantity\": [5.0,None,3.0,4.0,None]\n",
    "})\n",
    "\n",
    "# 11th scroll: Region, Area, Productivity\n",
    "scroll11 = pd.DataFrame({\n",
    "    \"Region\": [\"Andhra\",\"Karnataka\",\"TamilNadu\",\"Telangana\",\"Kerala\",\"Goa\",\"Maharashtra\",\n",
    "               \"Odisha\",\"Bihar\",\"Assam\"],\n",
    "    \"Area\": [146,131,90,94,107,134,115,123,83,120],\n",
    "    \"Productivity\": [47,66,68,54,44,61,60,48,48,65]\n",
    "})\n",
    "\n",
    "# 12th scroll: Name, Origin, Price\n",
    "scroll12 = pd.DataFrame({\n",
    "    \"Name\": [\"Arjun\",\"Bheem\",\"Chandra\",\"Drona\",\"Ekalavya\",\"Feroz\",\"Gopal\",\"Hari\",\"Indra\",\"Jai\"],\n",
    "    \"Origin\": [\"Arab\",\"Persian\",\"Fake\",\"Arab\",\"Arab\",\"Persian\",\"Fake\",\"Arab\",\"Persian\",\"Arab\"],\n",
    "    \"Price\": [3000,2500,500,4000,3500,2800,700,3200,2600,3900]\n",
    "})\n",
    "\n",
    "# 13th scroll: Roll, Marks\n",
    "scroll13 = pd.DataFrame({\n",
    "    \"Roll\": [101,102,104,106,107],\n",
    "    \"Marks\": [85,90,88,92,80]\n",
    "})\n",
    "\n",
    "# 14th scroll: Region, Rainfall\n",
    "scroll14 = pd.DataFrame({\n",
    "    \"Region\": [\"Vijayanagara\",\"Hampi\",\"Anegundi\",\"Kampli\",\"Tungabhadra\",\n",
    "               \"Badami\",\"Gulbarga\",\"Bellary\",\"Hospet\",\"Anantapur\"],\n",
    "    \"Rainfall\": [1079,1400,1506,1349,1102,1176,1571,1179,1241,1076]\n",
    "})\n",
    "\n",
    "# 15th scroll: Spice, Quantity, Price\n",
    "scroll15 = pd.DataFrame({\n",
    "    \"Spice\": [\"Pepper\",\"Cardamom\",\"Clove\",\"Cinnamon\",\"Nutmeg\",\"Turmeric\",\"Saffron\",\"Coriander\"],\n",
    "    \"Quantity\": [12.5,7.3,3.9,5.0,8.5,10.2,1.5,9.0],\n",
    "    \"Price\": [200,500,300,400,250,150,1000,120]\n",
    "})\n",
    "\n",
    "# 16th scroll: Stall, Category, Revenue\n",
    "scroll16 = pd.DataFrame({\n",
    "    \"Stall\": range(1,21),\n",
    "    \"Category\": [\"Sweets\",\"Horse\",\"Horse\",\"Horse\",\"Sweets\",\"Silk\",\"Silk\",\"Rice\",\"Rice\",\"Rice\",\n",
    "                 \"Silk\",\"Silk\",\"Horse\",\"Horse\",\"Rice\",\"Silk\",\"Rice\",\"Silk\",\"Rice\",\"Silk\"],\n",
    "    \"Revenue\": [4526,3121,2387,1647,1966,4998,790,3284,1279,669,\n",
    "                2710,3434,4490,2134,3093,1751,2241,2901,1852,4590]\n",
    "})\n",
    "\n",
    "# 17th scroll: Roll, Name\n",
    "scroll17 = pd.DataFrame({\n",
    "    \"Roll\": [101,102,103,104,105],\n",
    "    \"Name\": [\"Ravi\",\"Anand\",\"Meera\",\"Sita\",\"Hari\"]\n",
    "})\n",
    "\n",
    "# 18th scroll: Region, Value\n",
    "scroll18 = pd.DataFrame({\n",
    "    \"Region\": [\"Vijayanagara\",\"Hampi\",\"Anegundi\",\"Kampli\",\"Badami\"],\n",
    "    \"Value\": [500,450,300,400,350]\n",
    "})\n",
    "\n",
    "# 19th scroll: Donor, Amount\n",
    "scroll19 = pd.DataFrame({\n",
    "    \"Donor\": [\"Ravi\",\"Anand\",\"Meera\",\"Sita\",\"Hari\"],\n",
    "    \"Amount\": [500,700,600,800,550]\n",
    "})\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Merge all scrolls\n",
    "# -----------------------------\n",
    "scrolls = [scroll1, scroll2, scroll3, scroll4, scroll5, scroll6, scroll7, scroll8, scroll9,\n",
    "           scroll10, scroll11, scroll12, scroll13, scroll14, scroll15, scroll16, scroll17,\n",
    "           scroll18, scroll19]\n",
    "\n",
    "# Concatenate all scrolls into one master dataset\n",
    "master_df = pd.concat(scrolls, ignore_index=True, sort=False)\n",
    "\n",
    "# Drop duplicate rows and reset index\n",
    "master_df = master_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Step 3: Summarize Treasure_Value by Region\n",
    "# Some scrolls use \"Treasure_Value\", some use \"Value\"\n",
    "if \"Treasure_Value\" in master_df.columns:\n",
    "    master_df[\"Treasure_Value\"] = master_df[\"Treasure_Value\"].fillna(0)\n",
    "if \"Value\" in master_df.columns:\n",
    "    if \"Treasure_Value\" not in master_df.columns:\n",
    "        master_df[\"Treasure_Value\"] = master_df[\"Value\"].fillna(0)\n",
    "    else:\n",
    "        master_df[\"Treasure_Value\"] += master_df[\"Value\"].fillna(0)\n",
    "\n",
    "# Group by Region\n",
    "summary = master_df.groupby(\"Region\", as_index=False)[\"Treasure_Value\"].sum()\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Show results\n",
    "# -----------------------------\n",
    "#print(\"Master Dataset (first 10 rows):\\n\", master_df.head(10))\n",
    "print(\"\\nTreasure Value Summary by Region:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45289f24",
   "metadata": {},
   "source": [
    "Scene 20: Hidden Chamber of Hampi Finally, at the Virupaksha Temple, you decode the last riddle to reveal the treasure.\n",
    "\n",
    " Task: From the final dataset,\n",
    "\n",
    " compute: Region with maximum treasure Average treasure per category Treasure Key = sum of top 3 treasures\n",
    "\n",
    " Printing the treasure key unlocks the hidden wealth of Vijayanagara!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e90076e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region with Maximum Treasure: Vijayanagara (Total: 1100.0)\n",
      "\n",
      "Average Treasure per Category:\n",
      "   Category  Treasure_Value\n",
      "0    Horse             0.0\n",
      "1     Rice             0.0\n",
      "2     Silk             0.0\n",
      "3   Sweets             0.0\n",
      "\n",
      "Treasure Key (sum of top 3 treasures): 1550.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assume master_df already exists from Scene 19\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Ensure Treasure_Value column exists\n",
    "# -----------------------------\n",
    "if \"Treasure_Value\" not in master_df.columns:\n",
    "    if \"Value\" in master_df.columns:\n",
    "        master_df[\"Treasure_Value\"] = master_df[\"Value\"].fillna(0)\n",
    "    else:\n",
    "        master_df[\"Treasure_Value\"] = 0\n",
    "else:\n",
    "    master_df[\"Treasure_Value\"] = master_df[\"Treasure_Value\"].fillna(0)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Region with maximum treasure\n",
    "# -----------------------------\n",
    "region_max_treasure = master_df.groupby(\"Region\", as_index=False)[\"Treasure_Value\"].sum()\n",
    "max_region = region_max_treasure.loc[region_max_treasure[\"Treasure_Value\"].idxmax()]\n",
    "\n",
    "print(f\"Region with Maximum Treasure: {max_region['Region']} (Total: {max_region['Treasure_Value']})\")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Average treasure per category\n",
    "# -----------------------------\n",
    "if \"Category\" in master_df.columns:\n",
    "    avg_treasure_category = master_df.groupby(\"Category\", as_index=False)[\"Treasure_Value\"].mean()\n",
    "    print(\"\\nAverage Treasure per Category:\\n\", avg_treasure_category)\n",
    "else:\n",
    "    print(\"\\nNo 'Category' column found for average treasure computation.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Treasure Key = sum of top 3 treasures\n",
    "# -----------------------------\n",
    "top3_treasures = master_df[\"Treasure_Value\"].nlargest(3)\n",
    "treasure_key = top3_treasures.sum()\n",
    "print(f\"\\nTreasure Key (sum of top 3 treasures): {treasure_key}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PDV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
